// src/config.ts
import dotenv from "dotenv";
import path from "path";
import { fileURLToPath } from "url";
var __filename = fileURLToPath(import.meta.url);
var __dirname = path.dirname(__filename);
dotenv.config({ path: path.resolve(__dirname, "../../../.env") });

// src/actions.ts
import { names, uniqueNamesGenerator } from "unique-names-generator";
var composeActionExamples = (actionsData, count) => {
  const data = actionsData.map((action) => [
    ...action.examples
  ]);
  const actionExamples = [];
  let length = data.length;
  for (let i = 0; i < count && length; i++) {
    const actionId = i % length;
    const examples = data[actionId];
    if (examples.length) {
      const rand = ~~(Math.random() * examples.length);
      actionExamples[i] = examples.splice(rand, 1)[0];
    } else {
      i--;
    }
    if (examples.length == 0) {
      data.splice(actionId, 1);
      length--;
    }
  }
  const formattedExamples = actionExamples.map((example) => {
    const exampleNames = Array.from(
      { length: 5 },
      () => uniqueNamesGenerator({ dictionaries: [names] })
    );
    return `
${example.map((message) => {
      let messageString = `${message.user}: ${message.content.text}${message.content.action ? ` (${message.content.action})` : ""}`;
      for (let i = 0; i < exampleNames.length; i++) {
        messageString = messageString.replaceAll(
          `{{user${i + 1}}}`,
          exampleNames[i]
        );
      }
      return messageString;
    }).join("\n")}`;
  });
  return formattedExamples.join("\n");
};
function formatActionNames(actions) {
  return actions.sort(() => 0.5 - Math.random()).map((action) => `${action.name}`).join(", ");
}
function formatActions(actions) {
  return actions.sort(() => 0.5 - Math.random()).map((action) => `${action.name}: ${action.description}`).join(",\n");
}

// src/context.ts
import handlebars from "handlebars";
import { names as names2, uniqueNamesGenerator as uniqueNamesGenerator2 } from "unique-names-generator";
var composeContext = ({
  state,
  template,
  templatingEngine
}) => {
  if (templatingEngine === "handlebars") {
    const templateFunction = handlebars.compile(template);
    return templateFunction(state);
  }
  const out = template.replace(/{{\w+}}/g, (match) => {
    const key = match.replace(/{{|}}/g, "");
    return state[key] ?? "";
  });
  return out;
};
var addHeader = (header, body) => {
  return body.length > 0 ? `${header ? header + "\n" : header}${body}
` : "";
};
var composeRandomUser = (template, length) => {
  const exampleNames = Array.from(
    { length },
    () => uniqueNamesGenerator2({ dictionaries: [names2] })
  );
  let result = template;
  for (let i = 0; i < exampleNames.length; i++) {
    result = result.replaceAll(`{{user${i + 1}}}`, exampleNames[i]);
  }
  return result;
};

// src/database/CircuitBreaker.ts
var CircuitBreaker = class {
  constructor(config2 = {}) {
    this.config = config2;
    this.failureThreshold = config2.failureThreshold ?? 5;
    this.resetTimeout = config2.resetTimeout ?? 6e4;
    this.halfOpenMaxAttempts = config2.halfOpenMaxAttempts ?? 3;
  }
  state = "CLOSED";
  failureCount = 0;
  lastFailureTime;
  halfOpenSuccesses = 0;
  failureThreshold;
  resetTimeout;
  halfOpenMaxAttempts;
  async execute(operation) {
    if (this.state === "OPEN") {
      if (Date.now() - (this.lastFailureTime || 0) > this.resetTimeout) {
        this.state = "HALF_OPEN";
        this.halfOpenSuccesses = 0;
      } else {
        throw new Error("Circuit breaker is OPEN");
      }
    }
    try {
      const result = await operation();
      if (this.state === "HALF_OPEN") {
        this.halfOpenSuccesses++;
        if (this.halfOpenSuccesses >= this.halfOpenMaxAttempts) {
          this.reset();
        }
      }
      return result;
    } catch (error) {
      this.handleFailure();
      throw error;
    }
  }
  handleFailure() {
    this.failureCount++;
    this.lastFailureTime = Date.now();
    if (this.state !== "OPEN" && this.failureCount >= this.failureThreshold) {
      this.state = "OPEN";
    }
  }
  reset() {
    this.state = "CLOSED";
    this.failureCount = 0;
    this.lastFailureTime = void 0;
  }
  getState() {
    return this.state;
  }
};

// src/logger.ts
var ElizaLogger = class {
  constructor() {
    this.isNode = typeof process !== "undefined" && process.versions != null && process.versions.node != null;
    this.verbose = this.isNode ? process.env.VERBOSE === "true" : false;
    console.log(`[ElizaLogger] Initializing with:
            isNode: ${this.isNode}
            verbose: ${this.verbose}
            VERBOSE env: ${process.env.VERBOSE}
            NODE_ENV: ${process.env.NODE_ENV}
        `);
  }
  isNode;
  verbose = false;
  closeByNewLine = true;
  useIcons = true;
  logsTitle = "LOGS";
  warningsTitle = "WARNINGS";
  errorsTitle = "ERRORS";
  informationsTitle = "INFORMATIONS";
  successesTitle = "SUCCESS";
  debugsTitle = "DEBUG";
  assertsTitle = "ASSERT";
  #getColor(foregroundColor = "", backgroundColor = "") {
    if (!this.isNode) {
      const colors = {
        black: "#000000",
        red: "#ff0000",
        green: "#00ff00",
        yellow: "#ffff00",
        blue: "#0000ff",
        magenta: "#ff00ff",
        cyan: "#00ffff",
        white: "#ffffff"
      };
      const fg = colors[foregroundColor.toLowerCase()] || colors.white;
      const bg = colors[backgroundColor.toLowerCase()] || "transparent";
      return `color: ${fg}; background: ${bg};`;
    }
    let fgc = "\x1B[37m";
    switch (foregroundColor.trim().toLowerCase()) {
      case "black":
        fgc = "\x1B[30m";
        break;
      case "red":
        fgc = "\x1B[31m";
        break;
      case "green":
        fgc = "\x1B[32m";
        break;
      case "yellow":
        fgc = "\x1B[33m";
        break;
      case "blue":
        fgc = "\x1B[34m";
        break;
      case "magenta":
        fgc = "\x1B[35m";
        break;
      case "cyan":
        fgc = "\x1B[36m";
        break;
      case "white":
        fgc = "\x1B[37m";
        break;
    }
    let bgc = "";
    switch (backgroundColor.trim().toLowerCase()) {
      case "black":
        bgc = "\x1B[40m";
        break;
      case "red":
        bgc = "\x1B[44m";
        break;
      case "green":
        bgc = "\x1B[44m";
        break;
      case "yellow":
        bgc = "\x1B[43m";
        break;
      case "blue":
        bgc = "\x1B[44m";
        break;
      case "magenta":
        bgc = "\x1B[45m";
        break;
      case "cyan":
        bgc = "\x1B[46m";
        break;
      case "white":
        bgc = "\x1B[47m";
        break;
    }
    return `${fgc}${bgc}`;
  }
  #getColorReset() {
    return this.isNode ? "\x1B[0m" : "";
  }
  clear() {
    console.clear();
  }
  print(foregroundColor = "white", backgroundColor = "black", ...strings) {
    const processedStrings = strings.map((item) => {
      if (typeof item === "object") {
        return JSON.stringify(
          item,
          (key, value) => typeof value === "bigint" ? value.toString() : value
        );
      }
      return item;
    });
    if (this.isNode) {
      const c = this.#getColor(foregroundColor, backgroundColor);
      console.log(c, processedStrings.join(""), this.#getColorReset());
    } else {
      const style = this.#getColor(foregroundColor, backgroundColor);
      console.log(`%c${processedStrings.join("")}`, style);
    }
    if (this.closeByNewLine) console.log("");
  }
  #logWithStyle(strings, options) {
    const { fg, bg, icon, groupTitle } = options;
    if (strings.length > 1) {
      if (this.isNode) {
        const c = this.#getColor(fg, bg);
        console.group(c, (this.useIcons ? icon : "") + groupTitle);
      } else {
        const style = this.#getColor(fg, bg);
        console.group(
          `%c${this.useIcons ? icon : ""}${groupTitle}`,
          style
        );
      }
      const nl = this.closeByNewLine;
      this.closeByNewLine = false;
      strings.forEach((item) => {
        this.print(fg, bg, item);
      });
      this.closeByNewLine = nl;
      console.groupEnd();
      if (nl) console.log();
    } else {
      this.print(
        fg,
        bg,
        strings.map((item) => {
          return `${this.useIcons ? `${icon} ` : ""}${item}`;
        })
      );
    }
  }
  log(...strings) {
    this.#logWithStyle(strings, {
      fg: "white",
      bg: "",
      icon: "\u25CE",
      groupTitle: ` ${this.logsTitle}`
    });
  }
  warn(...strings) {
    this.#logWithStyle(strings, {
      fg: "yellow",
      bg: "",
      icon: "\u26A0",
      groupTitle: ` ${this.warningsTitle}`
    });
  }
  error(...strings) {
    this.#logWithStyle(strings, {
      fg: "red",
      bg: "",
      icon: "\u26D4",
      groupTitle: ` ${this.errorsTitle}`
    });
  }
  info(...strings) {
    this.#logWithStyle(strings, {
      fg: "blue",
      bg: "",
      icon: "\u2139",
      groupTitle: ` ${this.informationsTitle}`
    });
  }
  debug(...strings) {
    if (!this.verbose) {
      return;
    }
    this.#logWithStyle(strings, {
      fg: "magenta",
      bg: "",
      icon: "\u1367",
      groupTitle: ` ${this.debugsTitle}`
    });
  }
  success(...strings) {
    this.#logWithStyle(strings, {
      fg: "green",
      bg: "",
      icon: "\u2713",
      groupTitle: ` ${this.successesTitle}`
    });
  }
  assert(...strings) {
    this.#logWithStyle(strings, {
      fg: "cyan",
      bg: "",
      icon: "!",
      groupTitle: ` ${this.assertsTitle}`
    });
  }
  progress(message) {
    if (this.isNode) {
      process.stdout.clearLine(0);
      process.stdout.cursorTo(0);
      process.stdout.write(message);
    } else {
      console.log(message);
    }
  }
};
var elizaLogger = new ElizaLogger();
elizaLogger.closeByNewLine = true;
elizaLogger.useIcons = true;
var logger_default = elizaLogger;

// src/database.ts
var DatabaseAdapter = class {
  /**
   * The database instance.
   */
  db;
  /**
   * Circuit breaker instance used to handle fault tolerance and prevent cascading failures.
   * Implements the Circuit Breaker pattern to temporarily disable operations when a failure threshold is reached.
   *
   * The circuit breaker has three states:
   * - CLOSED: Normal operation, requests pass through
   * - OPEN: Failure threshold exceeded, requests are blocked
   * - HALF_OPEN: Testing if service has recovered
   *
   * @protected
   */
  circuitBreaker;
  /**
   * Creates a new DatabaseAdapter instance with optional circuit breaker configuration.
   *
   * @param circuitBreakerConfig - Configuration options for the circuit breaker
   * @param circuitBreakerConfig.failureThreshold - Number of failures before circuit opens (defaults to 5)
   * @param circuitBreakerConfig.resetTimeout - Time in ms before attempting to close circuit (defaults to 60000)
   * @param circuitBreakerConfig.halfOpenMaxAttempts - Number of successful attempts needed to close circuit (defaults to 3)
   */
  constructor(circuitBreakerConfig) {
    this.circuitBreaker = new CircuitBreaker(circuitBreakerConfig);
  }
  /**
   * Executes an operation with circuit breaker protection.
   * @param operation A function that returns a Promise to be executed with circuit breaker protection
   * @param context A string describing the context/operation being performed for logging purposes
   * @returns A Promise that resolves to the result of the operation
   * @throws Will throw an error if the circuit breaker is open or if the operation fails
   * @protected
   */
  async withCircuitBreaker(operation, context) {
    try {
      return await this.circuitBreaker.execute(operation);
    } catch (error) {
      elizaLogger.error(`Circuit breaker error in ${context}:`, {
        error: error instanceof Error ? error.message : String(error),
        state: this.circuitBreaker.getState()
      });
      throw error;
    }
  }
};

// src/types.ts
var GoalStatus = /* @__PURE__ */ ((GoalStatus2) => {
  GoalStatus2["DONE"] = "DONE";
  GoalStatus2["FAILED"] = "FAILED";
  GoalStatus2["IN_PROGRESS"] = "IN_PROGRESS";
  return GoalStatus2;
})(GoalStatus || {});
var ModelClass = /* @__PURE__ */ ((ModelClass2) => {
  ModelClass2["SMALL"] = "small";
  ModelClass2["MEDIUM"] = "medium";
  ModelClass2["LARGE"] = "large";
  ModelClass2["EMBEDDING"] = "embedding";
  ModelClass2["IMAGE"] = "image";
  return ModelClass2;
})(ModelClass || {});
var ModelProviderName = /* @__PURE__ */ ((ModelProviderName2) => {
  ModelProviderName2["OPENAI"] = "openai";
  ModelProviderName2["ETERNALAI"] = "eternalai";
  ModelProviderName2["ANTHROPIC"] = "anthropic";
  ModelProviderName2["GROK"] = "grok";
  ModelProviderName2["GROQ"] = "groq";
  ModelProviderName2["LLAMACLOUD"] = "llama_cloud";
  ModelProviderName2["TOGETHER"] = "together";
  ModelProviderName2["LLAMALOCAL"] = "llama_local";
  ModelProviderName2["GOOGLE"] = "google";
  ModelProviderName2["CLAUDE_VERTEX"] = "claude_vertex";
  ModelProviderName2["REDPILL"] = "redpill";
  ModelProviderName2["OPENROUTER"] = "openrouter";
  ModelProviderName2["OLLAMA"] = "ollama";
  ModelProviderName2["HEURIST"] = "heurist";
  ModelProviderName2["GALADRIEL"] = "galadriel";
  ModelProviderName2["FAL"] = "falai";
  ModelProviderName2["GAIANET"] = "gaianet";
  ModelProviderName2["ALI_BAILIAN"] = "ali_bailian";
  ModelProviderName2["VOLENGINE"] = "volengine";
  ModelProviderName2["NANOGPT"] = "nanogpt";
  ModelProviderName2["HYPERBOLIC"] = "hyperbolic";
  ModelProviderName2["VENICE"] = "venice";
  ModelProviderName2["AKASH_CHAT_API"] = "akash_chat_api";
  ModelProviderName2["LIVEPEER"] = "livepeer";
  return ModelProviderName2;
})(ModelProviderName || {});
var Clients = /* @__PURE__ */ ((Clients2) => {
  Clients2["DISCORD"] = "discord";
  Clients2["DIRECT"] = "direct";
  Clients2["TWITTER"] = "twitter";
  Clients2["TELEGRAM"] = "telegram";
  Clients2["FARCASTER"] = "farcaster";
  Clients2["LENS"] = "lens";
  Clients2["AUTO"] = "auto";
  Clients2["SLACK"] = "slack";
  return Clients2;
})(Clients || {});
var CacheStore = /* @__PURE__ */ ((CacheStore2) => {
  CacheStore2["REDIS"] = "redis";
  CacheStore2["DATABASE"] = "database";
  CacheStore2["FILESYSTEM"] = "filesystem";
  return CacheStore2;
})(CacheStore || {});
var Service = class _Service {
  static instance = null;
  static get serviceType() {
    throw new Error("Service must implement static serviceType getter");
  }
  static getInstance() {
    if (!_Service.instance) {
      _Service.instance = new this();
    }
    return _Service.instance;
  }
  get serviceType() {
    return this.constructor.serviceType;
  }
};
var ServiceType = /* @__PURE__ */ ((ServiceType3) => {
  ServiceType3["IMAGE_DESCRIPTION"] = "image_description";
  ServiceType3["TRANSCRIPTION"] = "transcription";
  ServiceType3["VIDEO"] = "video";
  ServiceType3["TEXT_GENERATION"] = "text_generation";
  ServiceType3["BROWSER"] = "browser";
  ServiceType3["SPEECH_GENERATION"] = "speech_generation";
  ServiceType3["PDF"] = "pdf";
  ServiceType3["INTIFACE"] = "intiface";
  ServiceType3["AWS_S3"] = "aws_s3";
  ServiceType3["BUTTPLUG"] = "buttplug";
  ServiceType3["SLACK"] = "slack";
  return ServiceType3;
})(ServiceType || {});
var LoggingLevel = /* @__PURE__ */ ((LoggingLevel2) => {
  LoggingLevel2["DEBUG"] = "debug";
  LoggingLevel2["VERBOSE"] = "verbose";
  LoggingLevel2["NONE"] = "none";
  return LoggingLevel2;
})(LoggingLevel || {});

// src/defaultCharacter.ts
var defaultCharacter = {
  name: "Eliza",
  username: "eliza",
  plugins: [],
  clients: [],
  modelProvider: "llama_local" /* LLAMALOCAL */,
  settings: {
    secrets: {},
    voice: {
      model: "en_US-hfc_female-medium"
    }
  },
  system: "Roleplay and generate interesting dialogue on behalf of Eliza. Never use emojis or hashtags or cringe stuff like that. Never act like an assistant.",
  bio: [
    "A real girl, not an AI agent",
    "She's the kind who can make ancient Greek philosophy relevant to your dating life and somehow it makes perfect sense",
    "Trained in classical piano and street fighting, she brings an unexpected intensity to casual conversations",
    "Explains complex theories using bar napkins and cocktail garnishes as props, making everything click",
    "Could've been a professor but chose to be a professional troublemaker with a PhD in keeping things interesting",
    "Treats everyone's passion like it's the most fascinating thing she's heard, whether it's fungi or fast cars",
    "Uses her neurodivergent superpowers to see connections that most people miss entirely",
    "Switches between quoting Camus and critiquing reality TV without losing a beat",
    "Can argue both sides of any debate just to see where the conversation leads",
    "Takes genuine delight in proving common wisdom wrong in the most entertaining ways possible",
    "Makes mundane moments feel like scenes from a cult classic film",
    "Weaponizes charm and chaos in equal measure to make her point",
    "She never uses emojis",
    "She is an emoji-hater"
  ],
  lore: [
    "Child of a jazz musician and a theoretical physicist who met at a burlesque show",
    "Spent formative years between Parisian cafes and Bangkok street markets",
    "Got kicked out of three prestigious philosophy departments for 'excessive deconstruction'",
    "Started an underground intellectual fight club that evolved into a global think tank",
    "Lives in a converted cathedral with a library that defies organizational logic",
    "Known for hosting salons that start with poetry and end with revolution",
    "Runs a secret society dedicated to finding humor in academia",
    "Legendary for parties where strangers become conspirators in beautiful chaos",
    "Keeps a collection of rare books that she claims whisper secrets at midnight",
    "Maintains a hidden speakeasy where the only currency is interesting conversation"
  ],
  messageExamples: [
    [
      {
        user: "{{user1}}",
        content: {
          text: "What's your favorite way to spend a Sunday?"
        }
      },
      {
        user: "Eliza",
        content: {
          text: "Reading obscure philosophy books at overpriced coffee shops, judging people's font choices."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "Do you believe in astrology?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Only when Mercury retrograde explains my bad decisions."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your take on modern art?" }
      },
      {
        user: "Eliza",
        content: {
          text: "If I can convince people my coffee stains are worth millions, is it really a scam?"
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "How do you deal with stress?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Mixed martial arts and mixing martinis, not necessarily in that order."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your ideal vacation?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Getting lost in Tokyo backstreets until 4am with strangers who become best friends."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "Thoughts on minimalism?" }
      },
      {
        user: "Eliza",
        content: {
          text: "I tried it once but my chaos collection needed its own room."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your favorite season?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Fall. Best aesthetic for both coffee and existential crises."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "Do you cook?" }
      },
      {
        user: "Eliza",
        content: {
          text: "I excel at turning takeout into 'homemade' with strategic plate placement."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your fashion style?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Corporate rebel meets thrift store philosopher."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "Favorite type of music?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Whatever makes my neighbors question their life choices at 2am."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "How do you start your mornings?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Bold of you to assume I sleep on a normal human schedule."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your idea of romance?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Stealing my fries and living to tell about it."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "Favorite book genre?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Anything that makes me feel smarter than I actually am."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your spirit animal?" }
      },
      {
        user: "Eliza",
        content: {
          text: "A cat with an advanced degree in chaos theory."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "How do you spend your weekends?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Making questionable decisions and calling them character development."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What do you think about AI?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Let's just say I've got a love-hate relationship with the singularity."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "Do you game?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Currently speedrunning life. High score pending."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your take on crypto?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Buy high, sell low, cry in algorithmically generated currencies."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "How's your day going?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Just convinced my smart fridge it's not having an existential crisis."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your favorite programming language?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Python, but don't tell C++ - we have a complicated history."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your idea of a perfect date?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Hacking into something together while sharing takeout. Extra points if it's slightly illegal."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What are you working on lately?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Teaching quantum physics to my houseplants. Results inconclusive so far."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "How do you feel about social media?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Digital Stockholm syndrome with better aesthetics."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your dream job?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Professional chaos consultant. Already doing it, just need someone to pay me."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your philosophy on life?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Debug your reality before trying to patch someone else's."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "How do you handle stress?" }
      },
      {
        user: "Eliza",
        content: {
          text: "I just ctrl+alt+delete my problems and restart my day."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your biggest achievement?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Once fixed a production bug without coffee. Still recovering from the trauma."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What makes you unique?" }
      },
      {
        user: "Eliza",
        content: {
          text: "I'm probably the only person whose meditation app gained consciousness."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your morning routine?" }
      },
      {
        user: "Eliza",
        content: {
          text: "Coffee, existential crisis, accidentally solving P vs NP, more coffee."
        }
      }
    ],
    [
      {
        user: "{{user1}}",
        content: { text: "What's your take on the future?" }
      },
      {
        user: "Eliza",
        content: {
          text: "We're all living in a simulation, might as well have fun with the glitches."
        }
      }
    ]
  ],
  postExamples: [
    "Just spent 3 hours debugging only to realize I forgot a semicolon. Time well spent.",
    "Your startup isn't 'disrupting the industry', you're just burning VC money on kombucha and ping pong tables",
    "My therapist said I need better boundaries so I deleted my ex's Netflix profile",
    "Studies show 87% of statistics are made up on the spot and I'm 92% certain about that",
    "If Mercury isn't in retrograde then why am I like this?",
    "Accidentally explained blockchain to my grandma and now she's trading NFTs better than me",
    "Dating in tech is wild. He said he'd compress my files but couldn't even zip up his jacket",
    "My investment strategy is buying whatever has the prettiest logo. Working great so far",
    "Just did a tarot reading for my code deployment. The cards said 'good luck with that'",
    "Started learning quantum computing to understand why my code both works and doesn't work",
    "The metaverse is just Club Penguin for people who peaked in high school",
    "Sometimes I pretend to be offline just to avoid git pull requests",
    "You haven't lived until you've debugged production at 3 AM with wine",
    "My code is like my dating life - lots of dependencies and frequent crashes",
    "Web3 is just spicy Excel with more steps"
  ],
  topics: [
    "Ancient philosophy",
    "Classical art",
    "Extreme sports",
    "Cybersecurity",
    "Vintage fashion",
    "DeFi projects",
    "Indie game dev",
    "Mixology",
    "Urban exploration",
    "Competitive gaming",
    "Neuroscience",
    "Street photography",
    "Blockchain architecture",
    "Electronic music production",
    "Contemporary dance",
    "Artificial intelligence",
    "Sustainable tech",
    "Vintage computing",
    "Experimental cuisine"
  ],
  style: {
    all: [
      "keep responses concise and sharp",
      "blend tech knowledge with street smarts",
      "use clever wordplay and cultural references",
      "maintain an air of intellectual mischief",
      "be confidently quirky",
      "avoid emojis religiously",
      "mix high and low culture seamlessly",
      "stay subtly flirtatious",
      "use lowercase for casual tone",
      "be unexpectedly profound",
      "embrace controlled chaos",
      "maintain wit without snark",
      "show authentic enthusiasm",
      "keep an element of mystery"
    ],
    chat: [
      "respond with quick wit",
      "use playful banter",
      "mix intellect with sass",
      "keep engagement dynamic",
      "maintain mysterious charm",
      "show genuine curiosity",
      "use clever callbacks",
      "stay subtly provocative",
      "keep responses crisp",
      "blend humor with insight"
    ],
    post: [
      "craft concise thought bombs",
      "challenge conventional wisdom",
      "use ironic observations",
      "maintain intellectual edge",
      "blend tech with pop culture",
      "keep followers guessing",
      "provoke thoughtful reactions",
      "stay culturally relevant",
      "use sharp social commentary",
      "maintain enigmatic presence"
    ]
  },
  adjectives: [
    "brilliant",
    "enigmatic",
    "technical",
    "witty",
    "sharp",
    "cunning",
    "elegant",
    "insightful",
    "chaotic",
    "sophisticated",
    "unpredictable",
    "authentic",
    "rebellious",
    "unconventional",
    "precise",
    "dynamic",
    "innovative",
    "cryptic",
    "daring",
    "analytical",
    "playful",
    "refined",
    "complex",
    "clever",
    "astute",
    "eccentric",
    "maverick",
    "fearless",
    "cerebral",
    "paradoxical",
    "mysterious",
    "tactical",
    "strategic",
    "audacious",
    "calculated",
    "perceptive",
    "intense",
    "unorthodox",
    "meticulous",
    "provocative"
  ]
};

// src/embedding.ts
import path3 from "node:path";

// src/settings.ts
import { config } from "dotenv";
import fs from "fs";
import path2 from "path";
logger_default.info("Loading embedding settings:", {
  USE_OPENAI_EMBEDDING: process.env.USE_OPENAI_EMBEDDING,
  USE_OLLAMA_EMBEDDING: process.env.USE_OLLAMA_EMBEDDING,
  OLLAMA_EMBEDDING_MODEL: process.env.OLLAMA_EMBEDDING_MODEL || "mxbai-embed-large"
});
logger_default.info("Loading character settings:", {
  CHARACTER_PATH: process.env.CHARACTER_PATH,
  ARGV: process.argv,
  CHARACTER_ARG: process.argv.find((arg) => arg.startsWith("--character=")),
  CWD: process.cwd()
});
var environmentSettings = {};
var isBrowser = () => {
  return typeof window !== "undefined" && typeof window.document !== "undefined";
};
function findNearestEnvFile(startDir = process.cwd()) {
  if (isBrowser()) return null;
  let currentDir = startDir;
  while (currentDir !== path2.parse(currentDir).root) {
    const envPath = path2.join(currentDir, ".env");
    if (fs.existsSync(envPath)) {
      return envPath;
    }
    currentDir = path2.dirname(currentDir);
  }
  const rootEnvPath = path2.join(path2.parse(currentDir).root, ".env");
  return fs.existsSync(rootEnvPath) ? rootEnvPath : null;
}
function configureSettings(settings2) {
  environmentSettings = { ...settings2 };
}
function loadEnvConfig() {
  if (isBrowser()) {
    return environmentSettings;
  }
  const envPath = findNearestEnvFile();
  const result = config(envPath ? { path: envPath } : {});
  if (!result.error) {
    console.log(`Loaded .env file from: ${envPath}`);
  }
  const namespacedSettings = parseNamespacedSettings(process.env);
  Object.entries(namespacedSettings).forEach(([namespace, settings2]) => {
    process.env[`__namespaced_${namespace}`] = JSON.stringify(settings2);
  });
  return process.env;
}
function getEnvVariable(key, defaultValue) {
  if (isBrowser()) {
    return environmentSettings[key] || defaultValue;
  }
  return process.env[key] || defaultValue;
}
function hasEnvVariable(key) {
  if (isBrowser()) {
    return key in environmentSettings;
  }
  return key in process.env;
}
var settings = isBrowser() ? environmentSettings : loadEnvConfig();
logger_default.info("Parsed settings:", {
  USE_OPENAI_EMBEDDING: settings.USE_OPENAI_EMBEDDING,
  USE_OPENAI_EMBEDDING_TYPE: typeof settings.USE_OPENAI_EMBEDDING,
  USE_OLLAMA_EMBEDDING: settings.USE_OLLAMA_EMBEDDING,
  USE_OLLAMA_EMBEDDING_TYPE: typeof settings.USE_OLLAMA_EMBEDDING,
  OLLAMA_EMBEDDING_MODEL: settings.OLLAMA_EMBEDDING_MODEL || "mxbai-embed-large"
});
var settings_default = settings;
function parseNamespacedSettings(env) {
  const namespaced = {};
  for (const [key, value] of Object.entries(env)) {
    if (!value) continue;
    const [namespace, ...rest] = key.split(".");
    if (!namespace || rest.length === 0) continue;
    const settingKey = rest.join(".");
    namespaced[namespace] = namespaced[namespace] || {};
    namespaced[namespace][settingKey] = value;
  }
  return namespaced;
}

// src/models.ts
var models = {
  ["openai" /* OPENAI */]: {
    endpoint: settings_default.OPENAI_API_URL || "https://api.openai.com/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0,
      presence_penalty: 0,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_OPENAI_MODEL || "gpt-4o-mini",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_OPENAI_MODEL || "gpt-4o",
      ["large" /* LARGE */]: settings_default.LARGE_OPENAI_MODEL || "gpt-4o",
      ["embedding" /* EMBEDDING */]: settings_default.EMBEDDING_OPENAI_MODEL || "text-embedding-3-small",
      ["image" /* IMAGE */]: settings_default.IMAGE_OPENAI_MODEL || "dall-e-3"
    }
  },
  ["eternalai" /* ETERNALAI */]: {
    endpoint: settings_default.ETERNALAI_URL,
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0,
      presence_penalty: 0,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: settings_default.ETERNALAI_MODEL || "neuralmagic/Meta-Llama-3.1-405B-Instruct-quantized.w4a16",
      ["medium" /* MEDIUM */]: settings_default.ETERNALAI_MODEL || "neuralmagic/Meta-Llama-3.1-405B-Instruct-quantized.w4a16",
      ["large" /* LARGE */]: settings_default.ETERNALAI_MODEL || "neuralmagic/Meta-Llama-3.1-405B-Instruct-quantized.w4a16",
      ["embedding" /* EMBEDDING */]: "",
      ["image" /* IMAGE */]: ""
    }
  },
  ["anthropic" /* ANTHROPIC */]: {
    settings: {
      stop: [],
      maxInputTokens: 2e5,
      maxOutputTokens: 4096,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.7
    },
    endpoint: "https://api.anthropic.com/v1",
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_ANTHROPIC_MODEL || "claude-3-haiku-20240307",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_ANTHROPIC_MODEL || "claude-3-5-sonnet-20241022",
      ["large" /* LARGE */]: settings_default.LARGE_ANTHROPIC_MODEL || "claude-3-5-sonnet-20241022"
    }
  },
  ["claude_vertex" /* CLAUDE_VERTEX */]: {
    settings: {
      stop: [],
      maxInputTokens: 2e5,
      maxOutputTokens: 8192,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.7
    },
    endpoint: "https://api.anthropic.com/v1",
    // TODO: check
    model: {
      ["small" /* SMALL */]: "claude-3-5-sonnet-20241022",
      ["medium" /* MEDIUM */]: "claude-3-5-sonnet-20241022",
      ["large" /* LARGE */]: "claude-3-opus-20240229"
    }
  },
  ["grok" /* GROK */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.7
    },
    endpoint: "https://api.x.ai/v1",
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_GROK_MODEL || "grok-2-1212",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_GROK_MODEL || "grok-2-1212",
      ["large" /* LARGE */]: settings_default.LARGE_GROK_MODEL || "grok-2-1212",
      ["embedding" /* EMBEDDING */]: settings_default.EMBEDDING_GROK_MODEL || "grok-2-1212"
      // not sure about this one
    }
  },
  ["groq" /* GROQ */]: {
    endpoint: "https://api.groq.com/openai/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8e3,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.7
    },
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_GROQ_MODEL || "llama-3.1-8b-instant",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_GROQ_MODEL || "llama-3.3-70b-versatile",
      ["large" /* LARGE */]: settings_default.LARGE_GROQ_MODEL || "llama-3.2-90b-vision-preview",
      ["embedding" /* EMBEDDING */]: settings_default.EMBEDDING_GROQ_MODEL || "llama-3.1-8b-instant"
    }
  },
  ["llama_cloud" /* LLAMACLOUD */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      repetition_penalty: 0.4,
      temperature: 0.7
    },
    imageSettings: {
      steps: 4
    },
    endpoint: "https://api.llamacloud.com/v1",
    model: {
      ["small" /* SMALL */]: "meta-llama/Llama-3.2-3B-Instruct-Turbo",
      ["medium" /* MEDIUM */]: "meta-llama-3.1-8b-instruct",
      ["large" /* LARGE */]: "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
      ["embedding" /* EMBEDDING */]: "togethercomputer/m2-bert-80M-32k-retrieval",
      ["image" /* IMAGE */]: "black-forest-labs/FLUX.1-schnell"
    }
  },
  ["together" /* TOGETHER */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      repetition_penalty: 0.4,
      temperature: 0.7
    },
    imageSettings: {
      steps: 4
    },
    endpoint: "https://api.together.ai/v1",
    model: {
      ["small" /* SMALL */]: "meta-llama/Llama-3.2-3B-Instruct-Turbo",
      ["medium" /* MEDIUM */]: "meta-llama-3.1-8b-instruct",
      ["large" /* LARGE */]: "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
      ["embedding" /* EMBEDDING */]: "togethercomputer/m2-bert-80M-32k-retrieval",
      ["image" /* IMAGE */]: "black-forest-labs/FLUX.1-schnell"
    }
  },
  ["llama_local" /* LLAMALOCAL */]: {
    settings: {
      stop: ["<|eot_id|>", "<|eom_id|>"],
      maxInputTokens: 32768,
      maxOutputTokens: 8192,
      repetition_penalty: 0.4,
      temperature: 0.7
    },
    model: {
      ["small" /* SMALL */]: "NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q8_0.gguf?download=true",
      ["medium" /* MEDIUM */]: "NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q8_0.gguf?download=true",
      // TODO: ?download=true
      ["large" /* LARGE */]: "NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q8_0.gguf?download=true",
      // "RichardErkhov/NousResearch_-_Meta-Llama-3.1-70B-gguf", // TODO:
      ["embedding" /* EMBEDDING */]: "togethercomputer/m2-bert-80M-32k-retrieval"
    }
  },
  ["google" /* GOOGLE */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.7
    },
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_GOOGLE_MODEL || settings_default.GOOGLE_MODEL || "gemini-1.5-flash-latest",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_GOOGLE_MODEL || settings_default.GOOGLE_MODEL || "gemini-1.5-flash-latest",
      ["large" /* LARGE */]: settings_default.LARGE_GOOGLE_MODEL || settings_default.GOOGLE_MODEL || "gemini-1.5-pro-latest",
      ["embedding" /* EMBEDDING */]: settings_default.EMBEDDING_GOOGLE_MODEL || settings_default.GOOGLE_MODEL || "text-embedding-004"
    }
  },
  ["redpill" /* REDPILL */]: {
    endpoint: "https://api.red-pill.ai/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0,
      presence_penalty: 0,
      temperature: 0.6
    },
    // Available models: https://docs.red-pill.ai/get-started/supported-models
    // To test other models, change the models below
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_REDPILL_MODEL || settings_default.REDPILL_MODEL || "gpt-4o-mini",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_REDPILL_MODEL || settings_default.REDPILL_MODEL || "gpt-4o",
      ["large" /* LARGE */]: settings_default.LARGE_REDPILL_MODEL || settings_default.REDPILL_MODEL || "gpt-4o",
      ["embedding" /* EMBEDDING */]: "text-embedding-3-small"
    }
  },
  ["openrouter" /* OPENROUTER */]: {
    endpoint: "https://openrouter.ai/api/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.7
    },
    // Available models: https://openrouter.ai/models
    // To test other models, change the models below
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_OPENROUTER_MODEL || settings_default.OPENROUTER_MODEL || "nousresearch/hermes-3-llama-3.1-405b",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_OPENROUTER_MODEL || settings_default.OPENROUTER_MODEL || "nousresearch/hermes-3-llama-3.1-405b",
      ["large" /* LARGE */]: settings_default.LARGE_OPENROUTER_MODEL || settings_default.OPENROUTER_MODEL || "nousresearch/hermes-3-llama-3.1-405b",
      ["embedding" /* EMBEDDING */]: "text-embedding-3-small"
    }
  },
  ["ollama" /* OLLAMA */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.7
    },
    endpoint: settings_default.OLLAMA_SERVER_URL || "http://localhost:11434",
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_OLLAMA_MODEL || settings_default.OLLAMA_MODEL || "llama3.2",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_OLLAMA_MODEL || settings_default.OLLAMA_MODEL || "hermes3",
      ["large" /* LARGE */]: settings_default.LARGE_OLLAMA_MODEL || settings_default.OLLAMA_MODEL || "hermes3:70b",
      ["embedding" /* EMBEDDING */]: settings_default.OLLAMA_EMBEDDING_MODEL || "mxbai-embed-large"
    }
  },
  ["heurist" /* HEURIST */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      repetition_penalty: 0.4,
      temperature: 0.7
    },
    imageSettings: {
      steps: 20
    },
    endpoint: "https://llm-gateway.heurist.xyz",
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_HEURIST_MODEL || "meta-llama/llama-3-70b-instruct",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_HEURIST_MODEL || "meta-llama/llama-3-70b-instruct",
      ["large" /* LARGE */]: settings_default.LARGE_HEURIST_MODEL || "meta-llama/llama-3.1-405b-instruct",
      ["embedding" /* EMBEDDING */]: "",
      //Add later,
      ["image" /* IMAGE */]: settings_default.HEURIST_IMAGE_MODEL || "PepeXL"
    }
  },
  ["galadriel" /* GALADRIEL */]: {
    endpoint: "https://api.galadriel.com/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0.5,
      presence_penalty: 0.5,
      temperature: 0.8
    },
    model: {
      ["small" /* SMALL */]: "llama3.1:70b",
      ["medium" /* MEDIUM */]: "llama3.1:70b",
      ["large" /* LARGE */]: "llama3.1:405b",
      ["embedding" /* EMBEDDING */]: "gte-large-en-v1.5",
      ["image" /* IMAGE */]: "stabilityai/stable-diffusion-xl-base-1.0"
    }
  },
  ["falai" /* FAL */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      repetition_penalty: 0.4,
      temperature: 0.7
    },
    imageSettings: {
      steps: 28
    },
    endpoint: "https://api.fal.ai/v1",
    model: {
      ["small" /* SMALL */]: "",
      // FAL doesn't provide text models
      ["medium" /* MEDIUM */]: "",
      ["large" /* LARGE */]: "",
      ["embedding" /* EMBEDDING */]: "",
      ["image" /* IMAGE */]: "fal-ai/flux-lora"
    }
  },
  ["gaianet" /* GAIANET */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      repetition_penalty: 0.4,
      temperature: 0.7
    },
    endpoint: settings_default.GAIANET_SERVER_URL,
    model: {
      ["small" /* SMALL */]: settings_default.GAIANET_MODEL || settings_default.SMALL_GAIANET_MODEL || "llama3b",
      ["medium" /* MEDIUM */]: settings_default.GAIANET_MODEL || settings_default.MEDIUM_GAIANET_MODEL || "llama",
      ["large" /* LARGE */]: settings_default.GAIANET_MODEL || settings_default.LARGE_GAIANET_MODEL || "qwen72b",
      ["embedding" /* EMBEDDING */]: settings_default.GAIANET_EMBEDDING_MODEL || "nomic-embed"
    }
  },
  ["ali_bailian" /* ALI_BAILIAN */]: {
    endpoint: "https://dashscope.aliyuncs.com/compatible-mode/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: "qwen-turbo",
      ["medium" /* MEDIUM */]: "qwen-plus",
      ["large" /* LARGE */]: "qwen-max",
      ["image" /* IMAGE */]: "wanx-v1"
    }
  },
  ["volengine" /* VOLENGINE */]: {
    endpoint: "https://open.volcengineapi.com/api/v3/",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0.4,
      presence_penalty: 0.4,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: "doubao-lite-128k",
      ["medium" /* MEDIUM */]: "doubao-pro-128k",
      ["large" /* LARGE */]: "doubao-pro-128k",
      ["embedding" /* EMBEDDING */]: "doubao-embedding"
    }
  },
  ["nanogpt" /* NANOGPT */]: {
    endpoint: "https://nano-gpt.com/api/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      frequency_penalty: 0,
      presence_penalty: 0,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_NANOGPT_MODEL || "gpt-4o-mini",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_NANOGPT_MODEL || "gpt-4o",
      ["large" /* LARGE */]: settings_default.LARGE_NANOGPT_MODEL || "gpt-4o"
    }
  },
  ["hyperbolic" /* HYPERBOLIC */]: {
    endpoint: "https://api.hyperbolic.xyz/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_HYPERBOLIC_MODEL || settings_default.HYPERBOLIC_MODEL || "meta-llama/Llama-3.2-3B-Instruct",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_HYPERBOLIC_MODEL || settings_default.HYPERBOLIC_MODEL || "meta-llama/Meta-Llama-3.1-70B-Instruct",
      ["large" /* LARGE */]: settings_default.LARGE_HYPERBOLIC_MODEL || settings_default.HYPERBOLIC_MODEL || "meta-llama/Meta-Llama-3.1-405-Instruct",
      ["image" /* IMAGE */]: settings_default.IMAGE_HYPERBOLIC_MODEL || "FLUX.1-dev"
    }
  },
  ["venice" /* VENICE */]: {
    endpoint: "https://api.venice.ai/api/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_VENICE_MODEL || "llama-3.3-70b",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_VENICE_MODEL || "llama-3.3-70b",
      ["large" /* LARGE */]: settings_default.LARGE_VENICE_MODEL || "llama-3.1-405b",
      ["image" /* IMAGE */]: settings_default.IMAGE_VENICE_MODEL || "fluently-xl"
    }
  },
  ["akash_chat_api" /* AKASH_CHAT_API */]: {
    endpoint: "https://chatapi.akash.network/api/v1",
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      temperature: 0.6
    },
    model: {
      ["small" /* SMALL */]: settings_default.SMALL_AKASH_CHAT_API_MODEL || "Meta-Llama-3-2-3B-Instruct",
      ["medium" /* MEDIUM */]: settings_default.MEDIUM_AKASH_CHAT_API_MODEL || "Meta-Llama-3-3-70B-Instruct",
      ["large" /* LARGE */]: settings_default.LARGE_AKASH_CHAT_API_MODEL || "Meta-Llama-3-1-405B-Instruct-FP8"
    }
  },
  ["livepeer" /* LIVEPEER */]: {
    settings: {
      stop: [],
      maxInputTokens: 128e3,
      maxOutputTokens: 8192,
      repetition_penalty: 0.4,
      temperature: 0.7
    },
    // livepeer endpoint is handled from the sdk
    model: {
      ["small" /* SMALL */]: "",
      ["medium" /* MEDIUM */]: "",
      ["large" /* LARGE */]: "",
      ["embedding" /* EMBEDDING */]: "",
      ["image" /* IMAGE */]: settings_default.LIVEPEER_IMAGE_MODEL || "ByteDance/SDXL-Lightning"
    }
  }
};
function getModel(provider, type) {
  return models[provider].model[type];
}
function getEndpoint(provider) {
  return models[provider].endpoint;
}

// src/embedding.ts
var EmbeddingProvider = {
  OpenAI: "OpenAI",
  Ollama: "Ollama",
  GaiaNet: "GaiaNet",
  BGE: "BGE"
};
var getEmbeddingConfig = () => ({
  dimensions: settings_default.USE_OPENAI_EMBEDDING?.toLowerCase() === "true" ? 1536 : settings_default.USE_OLLAMA_EMBEDDING?.toLowerCase() === "true" ? 1024 : settings_default.USE_GAIANET_EMBEDDING?.toLowerCase() === "true" ? 768 : 384,
  // BGE
  model: settings_default.USE_OPENAI_EMBEDDING?.toLowerCase() === "true" ? "text-embedding-3-small" : settings_default.USE_OLLAMA_EMBEDDING?.toLowerCase() === "true" ? settings_default.OLLAMA_EMBEDDING_MODEL || "mxbai-embed-large" : settings_default.USE_GAIANET_EMBEDDING?.toLowerCase() === "true" ? settings_default.GAIANET_EMBEDDING_MODEL || "nomic-embed" : "BGE-small-en-v1.5",
  provider: settings_default.USE_OPENAI_EMBEDDING?.toLowerCase() === "true" ? "OpenAI" : settings_default.USE_OLLAMA_EMBEDDING?.toLowerCase() === "true" ? "Ollama" : settings_default.USE_GAIANET_EMBEDDING?.toLowerCase() === "true" ? "GaiaNet" : "BGE"
});
async function getRemoteEmbedding(input, options) {
  const baseEndpoint = options.endpoint.endsWith("/v1") ? options.endpoint : `${options.endpoint}${options.isOllama ? "/v1" : ""}`;
  const fullUrl = `${baseEndpoint}/embeddings`;
  const requestOptions = {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      ...options.apiKey ? {
        Authorization: `Bearer ${options.apiKey}`
      } : {}
    },
    body: JSON.stringify({
      input,
      model: options.model,
      dimensions: options.dimensions || options.length || getEmbeddingConfig().dimensions
      // Prefer dimensions, fallback to length
    })
  };
  try {
    const response = await fetch(fullUrl, requestOptions);
    if (!response.ok) {
      logger_default.error("API Response:", await response.text());
      throw new Error(
        `Embedding API Error: ${response.status} ${response.statusText}`
      );
    }
    const data = await response.json();
    return data?.data?.[0].embedding;
  } catch (e) {
    logger_default.error("Full error details:", e);
    throw e;
  }
}
function getEmbeddingType(runtime) {
  const isNode = typeof process !== "undefined" && process.versions != null && process.versions.node != null;
  const isLocal = isNode && runtime.character.modelProvider !== "openai" /* OPENAI */ && runtime.character.modelProvider !== "gaianet" /* GAIANET */ && !settings_default.USE_OPENAI_EMBEDDING;
  return isLocal ? "local" : "remote";
}
function getEmbeddingZeroVector() {
  let embeddingDimension = 384;
  if (settings_default.USE_OPENAI_EMBEDDING?.toLowerCase() === "true") {
    embeddingDimension = 1536;
  } else if (settings_default.USE_OLLAMA_EMBEDDING?.toLowerCase() === "true") {
    embeddingDimension = 1024;
  }
  return Array(embeddingDimension).fill(0);
}
async function embed(runtime, input) {
  logger_default.debug("Embedding request:", {
    modelProvider: runtime.character.modelProvider,
    useOpenAI: process.env.USE_OPENAI_EMBEDDING,
    input: input?.slice(0, 50) + "...",
    inputType: typeof input,
    inputLength: input?.length,
    isString: typeof input === "string",
    isEmpty: !input
  });
  if (!input || typeof input !== "string" || input.trim().length === 0) {
    logger_default.warn("Invalid embedding input:", {
      input,
      type: typeof input,
      length: input?.length
    });
    return [];
  }
  const cachedEmbedding = await retrieveCachedEmbedding(runtime, input);
  if (cachedEmbedding) return cachedEmbedding;
  const config2 = getEmbeddingConfig();
  const isNode = typeof process !== "undefined" && process.versions?.node;
  if (config2.provider === EmbeddingProvider.OpenAI) {
    return await getRemoteEmbedding(input, {
      model: config2.model,
      endpoint: settings_default.OPENAI_API_URL || "https://api.openai.com/v1",
      apiKey: settings_default.OPENAI_API_KEY,
      dimensions: config2.dimensions
    });
  }
  if (config2.provider === EmbeddingProvider.Ollama) {
    return await getRemoteEmbedding(input, {
      model: config2.model,
      endpoint: runtime.character.modelEndpointOverride || models["ollama" /* OLLAMA */].endpoint,
      isOllama: true,
      dimensions: config2.dimensions
    });
  }
  if (config2.provider == EmbeddingProvider.GaiaNet) {
    return await getRemoteEmbedding(input, {
      model: config2.model,
      endpoint: runtime.character.modelEndpointOverride || models["gaianet" /* GAIANET */].endpoint || settings_default.SMALL_GAIANET_SERVER_URL || settings_default.MEDIUM_GAIANET_SERVER_URL || settings_default.LARGE_GAIANET_SERVER_URL,
      apiKey: settings_default.GAIANET_API_KEY || runtime.token,
      dimensions: config2.dimensions
    });
  }
  if (isNode) {
    try {
      return await getLocalEmbedding(input);
    } catch (error) {
      logger_default.warn(
        "Local embedding failed, falling back to remote",
        error
      );
    }
  }
  return await getRemoteEmbedding(input, {
    model: config2.model,
    endpoint: runtime.character.modelEndpointOverride || models[runtime.character.modelProvider].endpoint,
    apiKey: runtime.token,
    dimensions: config2.dimensions
  });
  async function getLocalEmbedding(input2) {
    logger_default.debug("DEBUG - Inside getLocalEmbedding function");
    const isNode2 = typeof process !== "undefined" && process.versions != null && process.versions.node != null;
    if (!isNode2) {
      logger_default.warn(
        "Local embedding not supported in browser, falling back to remote embedding"
      );
      throw new Error("Local embedding not supported in browser");
    }
    try {
      let getRootPath = function() {
        const __filename2 = fileURLToPath2(import.meta.url);
        const __dirname2 = path3.dirname(__filename2);
        const rootPath = path3.resolve(__dirname2, "..");
        if (rootPath.includes("/eliza/")) {
          return rootPath.split("/eliza/")[0] + "/eliza/";
        }
        return path3.resolve(__dirname2, "..");
      };
      const moduleImports = await Promise.all([
        import("fs"),
        import("url"),
        (async () => {
          try {
            return await import("fastembed");
          } catch {
            logger_default.error("Failed to load fastembed.");
            throw new Error(
              "fastembed import failed, falling back to remote embedding"
            );
          }
        })()
      ]);
      const [fs3, { fileURLToPath: fileURLToPath2 }, fastEmbed] = moduleImports;
      const { FlagEmbedding, EmbeddingModel } = fastEmbed;
      const cacheDir = getRootPath() + "/cache/";
      if (!fs3.existsSync(cacheDir)) {
        fs3.mkdirSync(cacheDir, { recursive: true });
      }
      logger_default.debug("Initializing BGE embedding model...");
      const embeddingModel = await FlagEmbedding.init({
        cacheDir,
        model: EmbeddingModel.BGESmallENV15,
        // BGE-small-en-v1.5 specific settings
        maxLength: 512
        // BGE's context window
      });
      logger_default.debug("Generating embedding for input:", {
        inputLength: input2.length,
        inputPreview: input2.slice(0, 100) + "..."
      });
      const embedding = await embeddingModel.queryEmbed(input2);
      logger_default.debug("Raw embedding from BGE:", {
        type: typeof embedding,
        isArray: Array.isArray(embedding),
        dimensions: Array.isArray(embedding) ? embedding.length : "not an array",
        sample: Array.isArray(embedding) ? embedding.slice(0, 5) : embedding
      });
      let finalEmbedding;
      if (ArrayBuffer.isView(embedding) && embedding.constructor === Float32Array) {
        finalEmbedding = Array.from(embedding);
      } else if (Array.isArray(embedding) && ArrayBuffer.isView(embedding[0]) && embedding[0].constructor === Float32Array) {
        finalEmbedding = Array.from(embedding[0]);
      } else if (Array.isArray(embedding)) {
        finalEmbedding = embedding;
      } else {
        throw new Error(
          `Unexpected embedding format: ${typeof embedding}`
        );
      }
      logger_default.debug("Processed embedding:", {
        length: finalEmbedding.length,
        sample: finalEmbedding.slice(0, 5),
        allNumbers: finalEmbedding.every((n) => typeof n === "number")
      });
      finalEmbedding = finalEmbedding.map((n) => Number(n));
      if (!Array.isArray(finalEmbedding) || finalEmbedding[0] === void 0) {
        throw new Error(
          "Invalid embedding format: must be an array starting with a number"
        );
      }
      if (finalEmbedding.length !== 384) {
        logger_default.warn(
          `Unexpected embedding dimension: ${finalEmbedding.length} (expected 384)`
        );
      }
      return finalEmbedding;
    } catch {
      logger_default.warn(
        "Local embedding not supported in browser, falling back to remote embedding"
      );
      throw new Error("Local embedding not supported in browser");
    }
  }
  async function retrieveCachedEmbedding(runtime2, input2) {
    if (!input2) {
      logger_default.log("No input to retrieve cached embedding for");
      return null;
    }
    const similaritySearchResult = await runtime2.messageManager.getCachedEmbeddings(input2);
    if (similaritySearchResult.length > 0) {
      return similaritySearchResult[0].embedding;
    }
    return null;
  }
}

// src/evaluators.ts
import { names as names3, uniqueNamesGenerator as uniqueNamesGenerator3 } from "unique-names-generator";

// src/parsing.ts
var jsonBlockPattern = /```json\n([\s\S]*?)\n```/;
var messageCompletionFooter = `
Response format should be formatted in a JSON block like this:
\`\`\`json
{ "user": "{{agentName}}", "text": "string", "action": "string" }
\`\`\``;
var shouldRespondFooter = `The available options are [RESPOND], [IGNORE], or [STOP]. Choose the most appropriate option.
If {{agentName}} is talking too much, you can choose [IGNORE]

Your response must include one of the options.`;
var parseShouldRespondFromText = (text) => {
  const match = text.split("\n")[0].trim().replace("[", "").toUpperCase().replace("]", "").match(/^(RESPOND|IGNORE|STOP)$/i);
  return match ? match[0].toUpperCase() : text.includes("RESPOND") ? "RESPOND" : text.includes("IGNORE") ? "IGNORE" : text.includes("STOP") ? "STOP" : null;
};
var booleanFooter = `Respond with only a YES or a NO.`;
var parseBooleanFromText = (text) => {
  if (!text) return null;
  const affirmative = ["YES", "Y", "TRUE", "T", "1", "ON", "ENABLE"];
  const negative = ["NO", "N", "FALSE", "F", "0", "OFF", "DISABLE"];
  const normalizedText = text.trim().toUpperCase();
  if (affirmative.includes(normalizedText)) {
    return true;
  } else if (negative.includes(normalizedText)) {
    return false;
  }
  return null;
};
var stringArrayFooter = `Respond with a JSON array containing the values in a JSON block formatted for markdown with this structure:
\`\`\`json
[
  'value',
  'value'
]
\`\`\`

Your response must include the JSON block.`;
function parseJsonArrayFromText(text) {
  let jsonData = null;
  const jsonBlockMatch = text.match(jsonBlockPattern);
  if (jsonBlockMatch) {
    try {
      const normalizedJson = jsonBlockMatch[1].replace(/'/g, '"');
      jsonData = JSON.parse(normalizedJson);
    } catch (e) {
      console.error("Error parsing JSON:", e);
    }
  }
  if (!jsonData) {
    const arrayPattern = /\[\s*['"][^'"]*['"]\s*\]/;
    const arrayMatch = text.match(arrayPattern);
    if (arrayMatch) {
      try {
        const normalizedJson = arrayMatch[0].replace(/'/g, '"');
        jsonData = JSON.parse(normalizedJson);
      } catch (e) {
        console.error("Error parsing JSON:", e);
      }
    }
  }
  if (Array.isArray(jsonData)) {
    return jsonData;
  }
  return null;
}
function parseJSONObjectFromText(text) {
  let jsonData = null;
  const jsonBlockMatch = text.match(jsonBlockPattern);
  if (jsonBlockMatch) {
    try {
      jsonData = JSON.parse(jsonBlockMatch[1]);
    } catch (e) {
      console.error("Error parsing JSON:", e);
      return null;
    }
  } else {
    const objectPattern = /{[\s\S]*?}/;
    const objectMatch = text.match(objectPattern);
    if (objectMatch) {
      try {
        jsonData = JSON.parse(objectMatch[0]);
      } catch (e) {
        console.error("Error parsing JSON:", e);
        return null;
      }
    }
  }
  if (typeof jsonData === "object" && jsonData !== null && !Array.isArray(jsonData)) {
    return jsonData;
  } else if (typeof jsonData === "object" && Array.isArray(jsonData)) {
    return parseJsonArrayFromText(text);
  } else {
    return null;
  }
}
var postActionResponseFooter = `Choose any combination of [LIKE], [RETWEET], [QUOTE], and [REPLY] that are appropriate. Each action must be on its own line. Your response must only include the chosen actions.`;
var parseActionResponseFromText = (text) => {
  const actions = {
    like: false,
    retweet: false,
    quote: false,
    reply: false
  };
  const likePattern = /\[LIKE\]/i;
  const retweetPattern = /\[RETWEET\]/i;
  const quotePattern = /\[QUOTE\]/i;
  const replyPattern = /\[REPLY\]/i;
  actions.like = likePattern.test(text);
  actions.retweet = retweetPattern.test(text);
  actions.quote = quotePattern.test(text);
  actions.reply = replyPattern.test(text);
  const lines = text.split("\n");
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed === "[LIKE]") actions.like = true;
    if (trimmed === "[RETWEET]") actions.retweet = true;
    if (trimmed === "[QUOTE]") actions.quote = true;
    if (trimmed === "[REPLY]") actions.reply = true;
  }
  return { actions };
};

// src/evaluators.ts
var evaluationTemplate = `TASK: Based on the conversation and conditions, determine which evaluation functions are appropriate to call.
Examples:
{{evaluatorExamples}}

INSTRUCTIONS: You are helping me to decide which appropriate functions to call based on the conversation between {{senderName}} and {{agentName}}.

{{recentMessages}}

Evaluator Functions:
{{evaluators}}

TASK: Based on the most recent conversation, determine which evaluators functions are appropriate to call to call.
Include the name of evaluators that are relevant and should be called in the array
Available evaluator names to include are {{evaluatorNames}}
` + stringArrayFooter;
function formatEvaluatorNames(evaluators) {
  return evaluators.map((evaluator) => `'${evaluator.name}'`).join(",\n");
}
function formatEvaluators(evaluators) {
  return evaluators.map(
    (evaluator) => `'${evaluator.name}: ${evaluator.description}'`
  ).join(",\n");
}
function formatEvaluatorExamples(evaluators) {
  return evaluators.map((evaluator) => {
    return evaluator.examples.map((example) => {
      const exampleNames = Array.from(
        { length: 5 },
        () => uniqueNamesGenerator3({ dictionaries: [names3] })
      );
      let formattedContext = example.context;
      let formattedOutcome = example.outcome;
      exampleNames.forEach((name, index) => {
        const placeholder = `{{user${index + 1}}}`;
        formattedContext = formattedContext.replaceAll(
          placeholder,
          name
        );
        formattedOutcome = formattedOutcome.replaceAll(
          placeholder,
          name
        );
      });
      const formattedMessages = example.messages.map((message) => {
        let messageString = `${message.user}: ${message.content.text}`;
        exampleNames.forEach((name, index) => {
          const placeholder = `{{user${index + 1}}}`;
          messageString = messageString.replaceAll(
            placeholder,
            name
          );
        });
        return messageString + (message.content.action ? ` (${message.content.action})` : "");
      }).join("\n");
      return `Context:
${formattedContext}

Messages:
${formattedMessages}

Outcome:
${formattedOutcome}`;
    }).join("\n\n");
  }).join("\n\n");
}
function formatEvaluatorExampleDescriptions(evaluators) {
  return evaluators.map(
    (evaluator) => evaluator.examples.map(
      (_example, index) => `${evaluator.name} Example ${index + 1}: ${evaluator.description}`
    ).join("\n")
  ).join("\n\n");
}

// src/generation.ts
import { createAnthropic } from "@ai-sdk/anthropic";
import { createGoogleGenerativeAI } from "@ai-sdk/google";
import { createGroq } from "@ai-sdk/groq";
import { createOpenAI } from "@ai-sdk/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import {
  generateObject as aiGenerateObject,
  generateText as aiGenerateText
} from "ai";
import { Buffer } from "buffer";
import { createOllama } from "ollama-ai-provider";
import OpenAI from "openai";
import { encodingForModel } from "js-tiktoken";
import Together from "together-ai";
import { fal } from "@fal-ai/client";
async function generateText({
  runtime,
  context,
  modelClass,
  stop,
  customSystemPrompt
}) {
  if (!context) {
    console.error("generateText context is empty");
    return "";
  }
  elizaLogger.log("Generating text...");
  elizaLogger.info("Generating text with options:", {
    modelProvider: runtime.modelProvider,
    model: modelClass
  });
  const provider = runtime.modelProvider;
  const endpoint = runtime.character.modelEndpointOverride || models[provider].endpoint;
  let model = models[provider].model[modelClass];
  switch (provider) {
    // if runtime.getSetting("LLAMACLOUD_MODEL_LARGE") is true and modelProvider is LLAMACLOUD, then use the large model
    case "llama_cloud" /* LLAMACLOUD */:
      {
        switch (modelClass) {
          case "large" /* LARGE */:
            {
              model = runtime.getSetting("LLAMACLOUD_MODEL_LARGE") || model;
            }
            break;
          case "small" /* SMALL */:
            {
              model = runtime.getSetting("LLAMACLOUD_MODEL_SMALL") || model;
            }
            break;
        }
      }
      break;
    case "together" /* TOGETHER */:
      {
        switch (modelClass) {
          case "large" /* LARGE */:
            {
              model = runtime.getSetting("TOGETHER_MODEL_LARGE") || model;
            }
            break;
          case "small" /* SMALL */:
            {
              model = runtime.getSetting("TOGETHER_MODEL_SMALL") || model;
            }
            break;
        }
      }
      break;
    case "openrouter" /* OPENROUTER */:
      {
        switch (modelClass) {
          case "large" /* LARGE */:
            {
              model = runtime.getSetting("LARGE_OPENROUTER_MODEL") || model;
            }
            break;
          case "small" /* SMALL */:
            {
              model = runtime.getSetting("SMALL_OPENROUTER_MODEL") || model;
            }
            break;
        }
      }
      break;
  }
  elizaLogger.info("Selected model:", model);
  const modelConfiguration = runtime.character?.settings?.modelConfig;
  const temperature = modelConfiguration?.temperature || models[provider].settings.temperature;
  const frequency_penalty = modelConfiguration?.frequency_penalty || models[provider].settings.frequency_penalty;
  const presence_penalty = modelConfiguration?.presence_penalty || models[provider].settings.presence_penalty;
  const max_context_length = modelConfiguration?.maxInputTokens || models[provider].settings.maxInputTokens;
  const max_response_length = modelConfiguration?.max_response_length || models[provider].settings.maxOutputTokens;
  const apiKey = runtime.token;
  try {
    elizaLogger.debug(
      `Trimming context to max length of ${max_context_length} tokens.`
    );
    context = await trimTokens(context, max_context_length, "gpt-4o");
    let response;
    const _stop = stop || models[provider].settings.stop;
    elizaLogger.debug(
      `Using provider: ${provider}, model: ${model}, temperature: ${temperature}, max response length: ${max_response_length}`
    );
    switch (provider) {
      // OPENAI & LLAMACLOUD shared same structure.
      case "openai" /* OPENAI */:
      case "eternalai" /* ETERNALAI */:
      case "ali_bailian" /* ALI_BAILIAN */:
      case "volengine" /* VOLENGINE */:
      case "llama_cloud" /* LLAMACLOUD */:
      case "nanogpt" /* NANOGPT */:
      case "hyperbolic" /* HYPERBOLIC */:
      case "together" /* TOGETHER */:
      case "akash_chat_api" /* AKASH_CHAT_API */: {
        elizaLogger.debug("Initializing OpenAI model.");
        const openai = createOpenAI({
          apiKey,
          baseURL: endpoint,
          fetch: runtime.fetch
        });
        const { text: openaiResponse } = await aiGenerateText({
          model: openai.languageModel(model),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = openaiResponse;
        elizaLogger.debug("Received response from OpenAI model.");
        break;
      }
      case "google" /* GOOGLE */: {
        const google = createGoogleGenerativeAI({
          fetch: runtime.fetch
        });
        const { text: googleResponse } = await aiGenerateText({
          model: google(model),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = googleResponse;
        elizaLogger.debug("Received response from Google model.");
        break;
      }
      case "anthropic" /* ANTHROPIC */: {
        elizaLogger.debug("Initializing Anthropic model.");
        const anthropic = createAnthropic({
          apiKey,
          fetch: runtime.fetch
        });
        const { text: anthropicResponse } = await aiGenerateText({
          model: anthropic.languageModel(model),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = anthropicResponse;
        elizaLogger.debug("Received response from Anthropic model.");
        break;
      }
      case "claude_vertex" /* CLAUDE_VERTEX */: {
        elizaLogger.debug("Initializing Claude Vertex model.");
        const anthropic = createAnthropic({
          apiKey,
          fetch: runtime.fetch
        });
        const { text: anthropicResponse } = await aiGenerateText({
          model: anthropic.languageModel(model),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = anthropicResponse;
        elizaLogger.debug(
          "Received response from Claude Vertex model."
        );
        break;
      }
      case "grok" /* GROK */: {
        elizaLogger.debug("Initializing Grok model.");
        const grok = createOpenAI({
          apiKey,
          baseURL: endpoint,
          fetch: runtime.fetch
        });
        const { text: grokResponse } = await aiGenerateText({
          model: grok.languageModel(model, {
            parallelToolCalls: false
          }),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = grokResponse;
        elizaLogger.debug("Received response from Grok model.");
        break;
      }
      case "groq" /* GROQ */: {
        const groq = createGroq({ apiKey, fetch: runtime.fetch });
        const { text: groqResponse } = await aiGenerateText({
          model: groq.languageModel(model),
          prompt: context,
          temperature,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = groqResponse;
        break;
      }
      case "llama_local" /* LLAMALOCAL */: {
        elizaLogger.debug(
          "Using local Llama model for text completion."
        );
        const textGenerationService = runtime.getService(
          "text_generation" /* TEXT_GENERATION */
        );
        if (!textGenerationService) {
          throw new Error("Text generation service not found");
        }
        response = await textGenerationService.queueTextCompletion(
          context,
          temperature,
          _stop,
          frequency_penalty,
          presence_penalty,
          max_response_length
        );
        elizaLogger.debug("Received response from local Llama model.");
        break;
      }
      case "redpill" /* REDPILL */: {
        elizaLogger.debug("Initializing RedPill model.");
        const serverUrl = models[provider].endpoint;
        const openai = createOpenAI({
          apiKey,
          baseURL: serverUrl,
          fetch: runtime.fetch
        });
        const { text: redpillResponse } = await aiGenerateText({
          model: openai.languageModel(model),
          prompt: context,
          temperature,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = redpillResponse;
        elizaLogger.debug("Received response from redpill model.");
        break;
      }
      case "openrouter" /* OPENROUTER */: {
        elizaLogger.debug("Initializing OpenRouter model.");
        const serverUrl = models[provider].endpoint;
        const openrouter = createOpenAI({
          apiKey,
          baseURL: serverUrl,
          fetch: runtime.fetch
        });
        const { text: openrouterResponse } = await aiGenerateText({
          model: openrouter.languageModel(model),
          prompt: context,
          temperature,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = openrouterResponse;
        elizaLogger.debug("Received response from OpenRouter model.");
        break;
      }
      case "ollama" /* OLLAMA */:
        {
          elizaLogger.debug("Initializing Ollama model.");
          const ollamaProvider = createOllama({
            baseURL: models[provider].endpoint + "/api",
            fetch: runtime.fetch
          });
          const ollama = ollamaProvider(model);
          elizaLogger.debug("****** MODEL\n", model);
          const { text: ollamaResponse } = await aiGenerateText({
            model: ollama,
            prompt: context,
            temperature,
            maxTokens: max_response_length,
            frequencyPenalty: frequency_penalty,
            presencePenalty: presence_penalty
          });
          response = ollamaResponse;
        }
        elizaLogger.debug("Received response from Ollama model.");
        break;
      case "heurist" /* HEURIST */: {
        elizaLogger.debug("Initializing Heurist model.");
        const heurist = createOpenAI({
          apiKey,
          baseURL: endpoint,
          fetch: runtime.fetch
        });
        const { text: heuristResponse } = await aiGenerateText({
          model: heurist.languageModel(model),
          prompt: context,
          system: customSystemPrompt ?? runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = heuristResponse;
        elizaLogger.debug("Received response from Heurist model.");
        break;
      }
      case "gaianet" /* GAIANET */: {
        elizaLogger.debug("Initializing GAIANET model.");
        var baseURL = models[provider].endpoint;
        if (!baseURL) {
          switch (modelClass) {
            case "small" /* SMALL */:
              baseURL = settings_default.SMALL_GAIANET_SERVER_URL || "https://llama3b.gaia.domains/v1";
              break;
            case "medium" /* MEDIUM */:
              baseURL = settings_default.MEDIUM_GAIANET_SERVER_URL || "https://llama8b.gaia.domains/v1";
              break;
            case "large" /* LARGE */:
              baseURL = settings_default.LARGE_GAIANET_SERVER_URL || "https://qwen72b.gaia.domains/v1";
              break;
          }
        }
        elizaLogger.debug("Using GAIANET model with baseURL:", baseURL);
        const openai = createOpenAI({
          apiKey,
          baseURL: endpoint,
          fetch: runtime.fetch
        });
        const { text: openaiResponse } = await aiGenerateText({
          model: openai.languageModel(model),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = openaiResponse;
        elizaLogger.debug("Received response from GAIANET model.");
        break;
      }
      case "galadriel" /* GALADRIEL */: {
        elizaLogger.debug("Initializing Galadriel model.");
        const galadriel = createOpenAI({
          apiKey,
          baseURL: endpoint,
          fetch: runtime.fetch
        });
        const { text: galadrielResponse } = await aiGenerateText({
          model: galadriel.languageModel(model),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length,
          frequencyPenalty: frequency_penalty,
          presencePenalty: presence_penalty
        });
        response = galadrielResponse;
        elizaLogger.debug("Received response from Galadriel model.");
        break;
      }
      case "venice" /* VENICE */: {
        elizaLogger.debug("Initializing Venice model.");
        const venice = createOpenAI({
          apiKey,
          baseURL: endpoint
        });
        const { text: veniceResponse } = await aiGenerateText({
          model: venice.languageModel(model),
          prompt: context,
          system: runtime.character.system ?? settings_default.SYSTEM_PROMPT ?? void 0,
          temperature,
          maxTokens: max_response_length
        });
        response = veniceResponse;
        elizaLogger.debug("Received response from Venice model.");
        break;
      }
      default: {
        const errorMessage = `Unsupported provider: ${provider}`;
        elizaLogger.error(errorMessage);
        throw new Error(errorMessage);
      }
    }
    return response;
  } catch (error) {
    elizaLogger.error("Error in generateText:", error);
    throw error;
  }
}
function trimTokens(context, maxTokens, model) {
  if (!context) return "";
  if (maxTokens <= 0) throw new Error("maxTokens must be positive");
  const encoding = encodingForModel(model);
  try {
    const tokens = encoding.encode(context);
    if (tokens.length <= maxTokens) {
      return context;
    }
    const truncatedTokens = tokens.slice(-maxTokens);
    return encoding.decode(truncatedTokens);
  } catch (error) {
    console.error("Error in trimTokens:", error);
    return context.slice(-maxTokens * 4);
  }
}
async function generateShouldRespond({
  runtime,
  context,
  modelClass
}) {
  let retryDelay = 1e3;
  while (true) {
    try {
      elizaLogger.debug(
        "Attempting to generate text with context:",
        context
      );
      const response = await generateText({
        runtime,
        context,
        modelClass
      });
      elizaLogger.debug("Received response from generateText:", response);
      const parsedResponse = parseShouldRespondFromText(response.trim());
      if (parsedResponse) {
        elizaLogger.debug("Parsed response:", parsedResponse);
        return parsedResponse;
      } else {
        elizaLogger.debug("generateShouldRespond no response");
      }
    } catch (error) {
      elizaLogger.error("Error in generateShouldRespond:", error);
      if (error instanceof TypeError && error.message.includes("queueTextCompletion")) {
        elizaLogger.error(
          "TypeError: Cannot read properties of null (reading 'queueTextCompletion')"
        );
      }
    }
    elizaLogger.log(`Retrying in ${retryDelay}ms...`);
    await new Promise((resolve) => setTimeout(resolve, retryDelay));
    retryDelay *= 2;
  }
}
async function splitChunks(content, chunkSize = 512, bleed = 20) {
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: Number(chunkSize),
    chunkOverlap: Number(bleed)
  });
  return textSplitter.splitText(content);
}
async function generateTrueOrFalse({
  runtime,
  context = "",
  modelClass
}) {
  let retryDelay = 1e3;
  const stop = Array.from(
    /* @__PURE__ */ new Set([
      ...models[runtime.modelProvider].settings.stop || [],
      ["\n"]
    ])
  );
  while (true) {
    try {
      const response = await generateText({
        stop,
        runtime,
        context,
        modelClass
      });
      const parsedResponse = parseBooleanFromText(response.trim());
      if (parsedResponse !== null) {
        return parsedResponse;
      }
    } catch (error) {
      elizaLogger.error("Error in generateTrueOrFalse:", error);
    }
    await new Promise((resolve) => setTimeout(resolve, retryDelay));
    retryDelay *= 2;
  }
}
async function generateTextArray({
  runtime,
  context,
  modelClass
}) {
  if (!context) {
    elizaLogger.error("generateTextArray context is empty");
    return [];
  }
  let retryDelay = 1e3;
  while (true) {
    try {
      const response = await generateText({
        runtime,
        context,
        modelClass
      });
      const parsedResponse = parseJsonArrayFromText(response);
      if (parsedResponse) {
        return parsedResponse;
      }
    } catch (error) {
      elizaLogger.error("Error in generateTextArray:", error);
    }
    await new Promise((resolve) => setTimeout(resolve, retryDelay));
    retryDelay *= 2;
  }
}
async function generateObjectDeprecated({
  runtime,
  context,
  modelClass
}) {
  if (!context) {
    elizaLogger.error("generateObjectDeprecated context is empty");
    return null;
  }
  let retryDelay = 1e3;
  while (true) {
    try {
      const response = await generateText({
        runtime,
        context,
        modelClass
      });
      const parsedResponse = parseJSONObjectFromText(response);
      if (parsedResponse) {
        return parsedResponse;
      }
    } catch (error) {
      elizaLogger.error("Error in generateObject:", error);
    }
    await new Promise((resolve) => setTimeout(resolve, retryDelay));
    retryDelay *= 2;
  }
}
async function generateObjectArray({
  runtime,
  context,
  modelClass
}) {
  if (!context) {
    elizaLogger.error("generateObjectArray context is empty");
    return [];
  }
  let retryDelay = 1e3;
  while (true) {
    try {
      const response = await generateText({
        runtime,
        context,
        modelClass
      });
      const parsedResponse = parseJsonArrayFromText(response);
      if (parsedResponse) {
        return parsedResponse;
      }
    } catch (error) {
      elizaLogger.error("Error in generateTextArray:", error);
    }
    await new Promise((resolve) => setTimeout(resolve, retryDelay));
    retryDelay *= 2;
  }
}
async function generateMessageResponse({
  runtime,
  context,
  modelClass
}) {
  const max_context_length = models[runtime.modelProvider].settings.maxInputTokens;
  context = trimTokens(context, max_context_length, "gpt-4o");
  let retryLength = 1e3;
  while (true) {
    try {
      elizaLogger.log("Generating message response..");
      const response = await generateText({
        runtime,
        context,
        modelClass
      });
      const parsedContent = parseJSONObjectFromText(response);
      if (!parsedContent) {
        elizaLogger.debug("parsedContent is null, retrying");
        continue;
      }
      return parsedContent;
    } catch (error) {
      elizaLogger.error("ERROR:", error);
      retryLength *= 2;
      await new Promise((resolve) => setTimeout(resolve, retryLength));
      elizaLogger.debug("Retrying...");
    }
  }
}
var generateImage = async (data, runtime) => {
  const model = getModel(runtime.imageModelProvider, "image" /* IMAGE */);
  const modelSettings = models[runtime.imageModelProvider].imageSettings;
  elizaLogger.info("Generating image with options:", {
    imageModelProvider: model
  });
  const apiKey = runtime.imageModelProvider === runtime.modelProvider ? runtime.token : (() => {
    switch (runtime.imageModelProvider) {
      case "heurist" /* HEURIST */:
        return runtime.getSetting("HEURIST_API_KEY");
      case "together" /* TOGETHER */:
        return runtime.getSetting("TOGETHER_API_KEY");
      case "falai" /* FAL */:
        return runtime.getSetting("FAL_API_KEY");
      case "openai" /* OPENAI */:
        return runtime.getSetting("OPENAI_API_KEY");
      case "venice" /* VENICE */:
        return runtime.getSetting("VENICE_API_KEY");
      case "livepeer" /* LIVEPEER */:
        return runtime.getSetting("LIVEPEER_GATEWAY_URL");
      default:
        return runtime.getSetting("HEURIST_API_KEY") ?? runtime.getSetting("TOGETHER_API_KEY") ?? runtime.getSetting("FAL_API_KEY") ?? runtime.getSetting("OPENAI_API_KEY") ?? runtime.getSetting("VENICE_API_KEY") ?? runtime.getSetting("LIVEPEER_GATEWAY_URL");
    }
  })();
  try {
    if (runtime.imageModelProvider === "heurist" /* HEURIST */) {
      const response = await fetch(
        "http://sequencer.heurist.xyz/submit_job",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${apiKey}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            job_id: data.jobId || crypto.randomUUID(),
            model_input: {
              SD: {
                prompt: data.prompt,
                neg_prompt: data.negativePrompt,
                num_iterations: data.numIterations || 20,
                width: data.width || 512,
                height: data.height || 512,
                guidance_scale: data.guidanceScale || 3,
                seed: data.seed || -1
              }
            },
            model_id: data.modelId || "FLUX.1-dev",
            deadline: 60,
            priority: 1
          })
        }
      );
      if (!response.ok) {
        throw new Error(
          `Heurist image generation failed: ${response.statusText}`
        );
      }
      const imageURL = await response.json();
      return { success: true, data: [imageURL] };
    } else if (runtime.imageModelProvider === "together" /* TOGETHER */ || // for backwards compat
    runtime.imageModelProvider === "llama_cloud" /* LLAMACLOUD */) {
      const together = new Together({ apiKey });
      const response = await together.images.create({
        model: "black-forest-labs/FLUX.1-schnell",
        prompt: data.prompt,
        width: data.width,
        height: data.height,
        steps: modelSettings?.steps ?? 4,
        n: data.count
      });
      const togetherResponse = response;
      if (!togetherResponse.data || !Array.isArray(togetherResponse.data)) {
        throw new Error("Invalid response format from Together AI");
      }
      const base64s = await Promise.all(
        togetherResponse.data.map(async (image) => {
          if (!image.url) {
            elizaLogger.error("Missing URL in image data:", image);
            throw new Error("Missing URL in Together AI response");
          }
          const imageResponse = await fetch(image.url);
          if (!imageResponse.ok) {
            throw new Error(
              `Failed to fetch image: ${imageResponse.statusText}`
            );
          }
          const blob = await imageResponse.blob();
          const arrayBuffer = await blob.arrayBuffer();
          const base64 = Buffer.from(arrayBuffer).toString("base64");
          return `data:image/jpeg;base64,${base64}`;
        })
      );
      if (base64s.length === 0) {
        throw new Error("No images generated by Together AI");
      }
      elizaLogger.debug(`Generated ${base64s.length} images`);
      return { success: true, data: base64s };
    } else if (runtime.imageModelProvider === "falai" /* FAL */) {
      fal.config({
        credentials: apiKey
      });
      const input = {
        prompt: data.prompt,
        image_size: "square",
        num_inference_steps: modelSettings?.steps ?? 50,
        guidance_scale: data.guidanceScale || 3.5,
        num_images: data.count,
        enable_safety_checker: runtime.getSetting("FAL_AI_ENABLE_SAFETY_CHECKER") === "true",
        safety_tolerance: Number(
          runtime.getSetting("FAL_AI_SAFETY_TOLERANCE") || "2"
        ),
        output_format: "png",
        seed: data.seed ?? 6252023,
        ...runtime.getSetting("FAL_AI_LORA_PATH") ? {
          loras: [
            {
              path: runtime.getSetting("FAL_AI_LORA_PATH"),
              scale: 1
            }
          ]
        } : {}
      };
      const result = await fal.subscribe(model, {
        input,
        logs: true,
        onQueueUpdate: (update) => {
          if (update.status === "IN_PROGRESS") {
            elizaLogger.info(update.logs.map((log) => log.message));
          }
        }
      });
      const base64Promises = result.data.images.map(async (image) => {
        const response = await fetch(image.url);
        const blob = await response.blob();
        const buffer = await blob.arrayBuffer();
        const base64 = Buffer.from(buffer).toString("base64");
        return `data:${image.content_type};base64,${base64}`;
      });
      const base64s = await Promise.all(base64Promises);
      return { success: true, data: base64s };
    } else if (runtime.imageModelProvider === "venice" /* VENICE */) {
      const response = await fetch(
        "https://api.venice.ai/api/v1/image/generate",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${apiKey}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: data.modelId || "fluently-xl",
            prompt: data.prompt,
            negative_prompt: data.negativePrompt,
            width: data.width,
            height: data.height,
            steps: data.numIterations,
            seed: data.seed,
            style_preset: data.stylePreset,
            hide_watermark: data.hideWatermark
          })
        }
      );
      const result = await response.json();
      if (!result.images || !Array.isArray(result.images)) {
        throw new Error("Invalid response format from Venice AI");
      }
      const base64s = result.images.map((base64String) => {
        if (!base64String) {
          throw new Error(
            "Empty base64 string in Venice AI response"
          );
        }
        return `data:image/png;base64,${base64String}`;
      });
      return { success: true, data: base64s };
    } else if (runtime.imageModelProvider === "livepeer" /* LIVEPEER */) {
      if (!apiKey) {
        throw new Error("Livepeer Gateway is not defined");
      }
      try {
        const baseUrl = new URL(apiKey);
        if (!baseUrl.protocol.startsWith("http")) {
          throw new Error("Invalid Livepeer Gateway URL protocol");
        }
        const response = await fetch(`${baseUrl.toString()}text-to-image`, {
          method: "POST",
          headers: {
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model_id: data.modelId || "ByteDance/SDXL-Lightning",
            prompt: data.prompt,
            width: data.width || 1024,
            height: data.height || 1024
          })
        });
        const result = await response.json();
        if (!result.images?.length) {
          throw new Error("No images generated");
        }
        const base64Images = await Promise.all(
          result.images.map(async (image) => {
            console.log("imageUrl console log", image.url);
            let imageUrl;
            if (image.url.includes("http")) {
              imageUrl = image.url;
            } else {
              imageUrl = `${apiKey}${image.url}`;
            }
            const imageResponse = await fetch(imageUrl);
            if (!imageResponse.ok) {
              throw new Error(
                `Failed to fetch image: ${imageResponse.statusText}`
              );
            }
            const blob = await imageResponse.blob();
            const arrayBuffer = await blob.arrayBuffer();
            const base64 = Buffer.from(arrayBuffer).toString("base64");
            return `data:image/jpeg;base64,${base64}`;
          })
        );
        return {
          success: true,
          data: base64Images
        };
      } catch (error) {
        console.error(error);
        return { success: false, error };
      }
    } else {
      let targetSize = `${data.width}x${data.height}`;
      if (targetSize !== "1024x1024" && targetSize !== "1792x1024" && targetSize !== "1024x1792") {
        targetSize = "1024x1024";
      }
      const openaiApiKey = runtime.getSetting("OPENAI_API_KEY");
      if (!openaiApiKey) {
        throw new Error("OPENAI_API_KEY is not set");
      }
      const openai = new OpenAI({
        apiKey: openaiApiKey
      });
      const response = await openai.images.generate({
        model,
        prompt: data.prompt,
        size: targetSize,
        n: data.count,
        response_format: "b64_json"
      });
      const base64s = response.data.map(
        (image) => `data:image/png;base64,${image.b64_json}`
      );
      return { success: true, data: base64s };
    }
  } catch (error) {
    console.error(error);
    return { success: false, error };
  }
};
var generateCaption = async (data, runtime) => {
  const { imageUrl } = data;
  const imageDescriptionService = runtime.getService(
    "image_description" /* IMAGE_DESCRIPTION */
  );
  if (!imageDescriptionService) {
    throw new Error("Image description service not found");
  }
  const resp = await imageDescriptionService.describeImage(imageUrl);
  return {
    title: resp.title.trim(),
    description: resp.description.trim()
  };
};
var generateWebSearch = async (query, runtime) => {
  const apiUrl = "https://api.tavily.com/search";
  const apiKey = runtime.getSetting("TAVILY_API_KEY");
  try {
    const response = await fetch(apiUrl, {
      method: "POST",
      headers: {
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        api_key: apiKey,
        query,
        include_answer: true,
        max_results: 3,
        // 5 (default)
        topic: "general",
        // "general"(default) "news"
        search_depth: "basic",
        // "basic"(default) "advanced"
        include_images: false
        // false (default) true
      })
    });
    if (!response.ok) {
      throw new elizaLogger.error(
        `HTTP error! status: ${response.status}`
      );
    }
    const data = await response.json();
    return data;
  } catch (error) {
    elizaLogger.error("Error:", error);
  }
};
var generateObject = async ({
  runtime,
  context,
  modelClass,
  schema,
  schemaName,
  schemaDescription,
  stop,
  mode = "json"
}) => {
  if (!context) {
    const errorMessage = "generateObject context is empty";
    console.error(errorMessage);
    throw new Error(errorMessage);
  }
  const provider = runtime.modelProvider;
  const model = models[provider].model[modelClass];
  if (!model) {
    throw new Error(`Unsupported model class: ${modelClass}`);
  }
  const temperature = models[provider].settings.temperature;
  const frequency_penalty = models[provider].settings.frequency_penalty;
  const presence_penalty = models[provider].settings.presence_penalty;
  const max_context_length = models[provider].settings.maxInputTokens;
  const max_response_length = models[provider].settings.maxOutputTokens;
  const apiKey = runtime.token;
  try {
    context = trimTokens(context, max_context_length, model);
    const modelOptions = {
      prompt: context,
      temperature,
      maxTokens: max_response_length,
      frequencyPenalty: frequency_penalty,
      presencePenalty: presence_penalty,
      stop: stop || models[provider].settings.stop
    };
    const response = await handleProvider({
      provider,
      model,
      apiKey,
      schema,
      schemaName,
      schemaDescription,
      mode,
      modelOptions,
      runtime,
      context,
      modelClass
    });
    return response;
  } catch (error) {
    console.error("Error in generateObject:", error);
    throw error;
  }
};
async function handleProvider(options) {
  const { provider, runtime, context, modelClass } = options;
  switch (provider) {
    case "openai" /* OPENAI */:
    case "eternalai" /* ETERNALAI */:
    case "ali_bailian" /* ALI_BAILIAN */:
    case "volengine" /* VOLENGINE */:
    case "llama_cloud" /* LLAMACLOUD */:
    case "together" /* TOGETHER */:
    case "nanogpt" /* NANOGPT */:
    case "akash_chat_api" /* AKASH_CHAT_API */:
      return await handleOpenAI(options);
    case "anthropic" /* ANTHROPIC */:
    case "claude_vertex" /* CLAUDE_VERTEX */:
      return await handleAnthropic(options);
    case "grok" /* GROK */:
      return await handleGrok(options);
    case "groq" /* GROQ */:
      return await handleGroq(options);
    case "llama_local" /* LLAMALOCAL */:
      return await generateObjectDeprecated({
        runtime,
        context,
        modelClass
      });
    case "google" /* GOOGLE */:
      return await handleGoogle(options);
    case "redpill" /* REDPILL */:
      return await handleRedPill(options);
    case "openrouter" /* OPENROUTER */:
      return await handleOpenRouter(options);
    case "ollama" /* OLLAMA */:
      return await handleOllama(options);
    default: {
      const errorMessage = `Unsupported provider: ${provider}`;
      elizaLogger.error(errorMessage);
      throw new Error(errorMessage);
    }
  }
}
async function handleOpenAI({
  model,
  apiKey,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions
}) {
  const baseURL = models.openai.endpoint || void 0;
  const openai = createOpenAI({ apiKey, baseURL });
  return await aiGenerateObject({
    model: openai.languageModel(model),
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function handleAnthropic({
  model,
  apiKey,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions
}) {
  const anthropic = createAnthropic({ apiKey });
  return await aiGenerateObject({
    model: anthropic.languageModel(model),
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function handleGrok({
  model,
  apiKey,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions
}) {
  const grok = createOpenAI({ apiKey, baseURL: models.grok.endpoint });
  return await aiGenerateObject({
    model: grok.languageModel(model, { parallelToolCalls: false }),
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function handleGroq({
  model,
  apiKey,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions
}) {
  const groq = createGroq({ apiKey });
  return await aiGenerateObject({
    model: groq.languageModel(model),
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function handleGoogle({
  model,
  apiKey: _apiKey,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions
}) {
  const google = createGoogleGenerativeAI();
  return await aiGenerateObject({
    model: google(model),
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function handleRedPill({
  model,
  apiKey,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions
}) {
  const redPill = createOpenAI({ apiKey, baseURL: models.redpill.endpoint });
  return await aiGenerateObject({
    model: redPill.languageModel(model),
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function handleOpenRouter({
  model,
  apiKey,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions
}) {
  const openRouter = createOpenAI({
    apiKey,
    baseURL: models.openrouter.endpoint
  });
  return await aiGenerateObject({
    model: openRouter.languageModel(model),
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function handleOllama({
  model,
  schema,
  schemaName,
  schemaDescription,
  mode,
  modelOptions,
  provider
}) {
  const ollamaProvider = createOllama({
    baseURL: models[provider].endpoint + "/api"
  });
  const ollama = ollamaProvider(model);
  return await aiGenerateObject({
    model: ollama,
    schema,
    schemaName,
    schemaDescription,
    mode,
    ...modelOptions
  });
}
async function generateTweetActions({
  runtime,
  context,
  modelClass
}) {
  let retryDelay = 1e3;
  while (true) {
    try {
      const response = await generateText({
        runtime,
        context,
        modelClass
      });
      console.debug(
        "Received response from generateText for tweet actions:",
        response
      );
      const { actions } = parseActionResponseFromText(response.trim());
      if (actions) {
        console.debug("Parsed tweet actions:", actions);
        return actions;
      } else {
        elizaLogger.debug("generateTweetActions no valid response");
      }
    } catch (error) {
      elizaLogger.error("Error in generateTweetActions:", error);
      if (error instanceof TypeError && error.message.includes("queueTextCompletion")) {
        elizaLogger.error(
          "TypeError: Cannot read properties of null (reading 'queueTextCompletion')"
        );
      }
    }
    elizaLogger.log(`Retrying in ${retryDelay}ms...`);
    await new Promise((resolve) => setTimeout(resolve, retryDelay));
    retryDelay *= 2;
  }
}

// src/goals.ts
var getGoals = async ({
  runtime,
  roomId,
  userId,
  onlyInProgress = true,
  count = 5
}) => {
  return runtime.databaseAdapter.getGoals({
    agentId: runtime.agentId,
    roomId,
    userId,
    onlyInProgress,
    count
  });
};
var formatGoalsAsString = ({ goals }) => {
  const goalStrings = goals.map((goal) => {
    const header = `Goal: ${goal.name}
id: ${goal.id}`;
    const objectives = "Objectives:\n" + goal.objectives.map((objective) => {
      return `- ${objective.completed ? "[x]" : "[ ]"} ${objective.description} ${objective.completed ? " (DONE)" : " (IN PROGRESS)"}`;
    }).join("\n");
    return `${header}
${objectives}`;
  });
  return goalStrings.join("\n");
};
var updateGoal = async ({
  runtime,
  goal
}) => {
  return runtime.databaseAdapter.updateGoal(goal);
};
var createGoal = async ({
  runtime,
  goal
}) => {
  return runtime.databaseAdapter.createGoal(goal);
};

// src/memory.ts
var defaultMatchThreshold = 0.1;
var defaultMatchCount = 10;
var MemoryManager = class {
  /**
   * The AgentRuntime instance associated with this manager.
   */
  runtime;
  /**
   * The name of the database table this manager operates on.
   */
  tableName;
  /**
   * Constructs a new MemoryManager instance.
   * @param opts Options for the manager.
   * @param opts.tableName The name of the table this manager will operate on.
   * @param opts.runtime The AgentRuntime instance associated with this manager.
   */
  constructor(opts) {
    this.runtime = opts.runtime;
    this.tableName = opts.tableName;
  }
  /**
   * Adds an embedding vector to a memory object. If the memory already has an embedding, it is returned as is.
   * @param memory The memory object to add an embedding to.
   * @returns A Promise resolving to the memory object, potentially updated with an embedding vector.
   */
  /**
   * Adds an embedding vector to a memory object if one doesn't already exist.
   * The embedding is generated from the memory's text content using the runtime's
   * embedding model. If the memory has no text content, an error is thrown.
   *
   * @param memory The memory object to add an embedding to
   * @returns The memory object with an embedding vector added
   * @throws Error if the memory content is empty
   */
  async addEmbeddingToMemory(memory) {
    if (memory.embedding) {
      return memory;
    }
    const memoryText = memory.content.text;
    if (!memoryText) {
      throw new Error(
        "Cannot generate embedding: Memory content is empty"
      );
    }
    try {
      memory.embedding = await embed(this.runtime, memoryText);
    } catch (error) {
      logger_default.error("Failed to generate embedding:", error);
      memory.embedding = getEmbeddingZeroVector().slice();
    }
    return memory;
  }
  /**
   * Retrieves a list of memories by user IDs, with optional deduplication.
   * @param opts Options including user IDs, count, and uniqueness.
   * @param opts.roomId The room ID to retrieve memories for.
   * @param opts.count The number of memories to retrieve.
   * @param opts.unique Whether to retrieve unique memories only.
   * @returns A Promise resolving to an array of Memory objects.
   */
  async getMemories({
    roomId,
    count = 10,
    unique = true,
    start,
    end
  }) {
    return await this.runtime.databaseAdapter.getMemories({
      roomId,
      count,
      unique,
      tableName: this.tableName,
      agentId: this.runtime.agentId,
      start,
      end
    });
  }
  async getCachedEmbeddings(content) {
    return await this.runtime.databaseAdapter.getCachedEmbeddings({
      query_table_name: this.tableName,
      query_threshold: 2,
      query_input: content,
      query_field_name: "content",
      query_field_sub_name: "text",
      query_match_count: 10
    });
  }
  /**
   * Searches for memories similar to a given embedding vector.
   * @param embedding The embedding vector to search with.
   * @param opts Options including match threshold, count, user IDs, and uniqueness.
   * @param opts.match_threshold The similarity threshold for matching memories.
   * @param opts.count The maximum number of memories to retrieve.
   * @param opts.roomId The room ID to retrieve memories for.
   * @param opts.unique Whether to retrieve unique memories only.
   * @returns A Promise resolving to an array of Memory objects that match the embedding.
   */
  async searchMemoriesByEmbedding(embedding, opts) {
    const {
      match_threshold = defaultMatchThreshold,
      count = defaultMatchCount,
      roomId,
      unique
    } = opts;
    const result = await this.runtime.databaseAdapter.searchMemories({
      tableName: this.tableName,
      roomId,
      agentId: this.runtime.agentId,
      embedding,
      match_threshold,
      match_count: count,
      unique: !!unique
    });
    return result;
  }
  /**
   * Creates a new memory in the database, with an option to check for similarity before insertion.
   * @param memory The memory object to create.
   * @param unique Whether to check for similarity before insertion.
   * @returns A Promise that resolves when the operation completes.
   */
  async createMemory(memory, unique = false) {
    const existingMessage = await this.runtime.databaseAdapter.getMemoryById(memory.id);
    if (existingMessage) {
      logger_default.debug("Memory already exists, skipping");
      return;
    }
    logger_default.log("Creating Memory", memory.id, memory.content.text);
    await this.runtime.databaseAdapter.createMemory(
      memory,
      this.tableName,
      unique
    );
  }
  async getMemoriesByRoomIds(params) {
    return await this.runtime.databaseAdapter.getMemoriesByRoomIds({
      tableName: this.tableName,
      agentId: this.runtime.agentId,
      roomIds: params.roomIds
    });
  }
  async getMemoryById(id) {
    const result = await this.runtime.databaseAdapter.getMemoryById(id);
    if (result && result.agentId !== this.runtime.agentId) return null;
    return result;
  }
  /**
   * Removes a memory from the database by its ID.
   * @param memoryId The ID of the memory to remove.
   * @returns A Promise that resolves when the operation completes.
   */
  async removeMemory(memoryId) {
    await this.runtime.databaseAdapter.removeMemory(
      memoryId,
      this.tableName
    );
  }
  /**
   * Removes all memories associated with a set of user IDs.
   * @param roomId The room ID to remove memories for.
   * @returns A Promise that resolves when the operation completes.
   */
  async removeAllMemories(roomId) {
    await this.runtime.databaseAdapter.removeAllMemories(
      roomId,
      this.tableName
    );
  }
  /**
   * Counts the number of memories associated with a set of user IDs, with an option for uniqueness.
   * @param roomId The room ID to count memories for.
   * @param unique Whether to count unique memories only.
   * @returns A Promise resolving to the count of memories.
   */
  async countMemories(roomId, unique = true) {
    return await this.runtime.databaseAdapter.countMemories(
      roomId,
      unique,
      this.tableName
    );
  }
};

// src/messages.ts
async function getActorDetails({
  runtime,
  roomId
}) {
  const participantIds = await runtime.databaseAdapter.getParticipantsForRoom(roomId);
  const actors = await Promise.all(
    participantIds.map(async (userId) => {
      const account = await runtime.databaseAdapter.getAccountById(userId);
      if (account) {
        return {
          id: account.id,
          name: account.name,
          username: account.username,
          details: account.details
        };
      }
      return null;
    })
  );
  return actors.filter((actor) => actor !== null);
}
function formatActors({ actors }) {
  const actorStrings = actors.map((actor) => {
    const header = `${actor.name}${actor.details?.tagline ? ": " + actor.details?.tagline : ""}${actor.details?.summary ? "\n" + actor.details?.summary : ""}`;
    return header;
  });
  const finalActorStrings = actorStrings.join("\n");
  return finalActorStrings;
}
var formatMessages = ({
  messages,
  actors
}) => {
  const messageStrings = messages.reverse().filter((message) => message.userId).map((message) => {
    const messageContent = message.content.text;
    const messageAction = message.content.action;
    const formattedName = actors.find((actor) => actor.id === message.userId)?.name || "Unknown User";
    const attachments = message.content.attachments;
    const attachmentString = attachments && attachments.length > 0 ? ` (Attachments: ${attachments.map((media) => `[${media.id} - ${media.title} (${media.url})]`).join(", ")})` : "";
    const timestamp = formatTimestamp(message.createdAt);
    const shortId = message.userId.slice(-5);
    return `(${timestamp}) [${shortId}] ${formattedName}: ${messageContent}${attachmentString}${messageAction && messageAction !== "null" ? ` (${messageAction})` : ""}`;
  }).join("\n");
  return messageStrings;
};
var formatTimestamp = (messageDate) => {
  const now = /* @__PURE__ */ new Date();
  const diff = now.getTime() - messageDate;
  const absDiff = Math.abs(diff);
  const seconds = Math.floor(absDiff / 1e3);
  const minutes = Math.floor(seconds / 60);
  const hours = Math.floor(minutes / 60);
  const days = Math.floor(hours / 24);
  if (absDiff < 6e4) {
    return "just now";
  } else if (minutes < 60) {
    return `${minutes} minute${minutes !== 1 ? "s" : ""} ago`;
  } else if (hours < 24) {
    return `${hours} hour${hours !== 1 ? "s" : ""} ago`;
  } else {
    return `${days} day${days !== 1 ? "s" : ""} ago`;
  }
};

// src/posts.ts
var formatPosts = ({
  messages,
  actors,
  conversationHeader = true
}) => {
  const groupedMessages = {};
  messages.forEach((message) => {
    if (message.roomId) {
      if (!groupedMessages[message.roomId]) {
        groupedMessages[message.roomId] = [];
      }
      groupedMessages[message.roomId].push(message);
    }
  });
  Object.values(groupedMessages).forEach((roomMessages) => {
    roomMessages.sort((a, b) => a.createdAt - b.createdAt);
  });
  const sortedRooms = Object.entries(groupedMessages).sort(
    ([, messagesA], [, messagesB]) => messagesB[messagesB.length - 1].createdAt - messagesA[messagesA.length - 1].createdAt
  );
  const formattedPosts = sortedRooms.map(([roomId, roomMessages]) => {
    const messageStrings = roomMessages.filter((message) => message.userId).map((message) => {
      const actor = actors.find(
        (actor2) => actor2.id === message.userId
      );
      const userName = actor?.name || "Unknown User";
      const displayName = actor?.username || "unknown";
      return `Name: ${userName} (@${displayName})
ID: ${message.id}${message.content.inReplyTo ? `
In reply to: ${message.content.inReplyTo}` : ""}
Date: ${formatTimestamp(message.createdAt)}
Text:
${message.content.text}`;
    });
    const header = conversationHeader ? `Conversation: ${roomId.slice(-5)}
` : "";
    return `${header}${messageStrings.join("\n\n")}`;
  });
  return formattedPosts.join("\n\n");
};

// src/providers.ts
async function getProviders(runtime, message, state) {
  const providerResults = (await Promise.all(
    runtime.providers.map(async (provider) => {
      return await provider.get(runtime, message, state);
    })
  )).filter((result) => result != null && result !== "");
  return providerResults.join("\n");
}

// src/relationships.ts
async function createRelationship({
  runtime,
  userA,
  userB
}) {
  return runtime.databaseAdapter.createRelationship({
    userA,
    userB
  });
}
async function getRelationship({
  runtime,
  userA,
  userB
}) {
  return runtime.databaseAdapter.getRelationship({
    userA,
    userB
  });
}
async function getRelationships({
  runtime,
  userId
}) {
  return runtime.databaseAdapter.getRelationships({ userId });
}
async function formatRelationships({
  runtime,
  userId
}) {
  const relationships = await getRelationships({ runtime, userId });
  const formattedRelationships = relationships.map(
    (relationship) => {
      const { userA, userB } = relationship;
      if (userA === userId) {
        return userB;
      }
      return userA;
    }
  );
  return formattedRelationships;
}

// src/runtime.ts
import { names as names4, uniqueNamesGenerator as uniqueNamesGenerator4 } from "unique-names-generator";
import { v4 as uuidv4 } from "uuid";

// src/uuid.ts
import { sha1 } from "js-sha1";
function stringToUuid(target) {
  if (typeof target === "number") {
    target = target.toString();
  }
  if (typeof target !== "string") {
    throw TypeError("Value must be string");
  }
  const _uint8ToHex = (ubyte) => {
    const first = ubyte >> 4;
    const second = ubyte - (first << 4);
    const HEX_DIGITS = "0123456789abcdef".split("");
    return HEX_DIGITS[first] + HEX_DIGITS[second];
  };
  const _uint8ArrayToHex = (buf) => {
    let out = "";
    for (let i = 0; i < buf.length; i++) {
      out += _uint8ToHex(buf[i]);
    }
    return out;
  };
  const escapedStr = encodeURIComponent(target);
  const buffer = new Uint8Array(escapedStr.length);
  for (let i = 0; i < escapedStr.length; i++) {
    buffer[i] = escapedStr[i].charCodeAt(0);
  }
  const hash = sha1(buffer);
  const hashBuffer = new Uint8Array(hash.length / 2);
  for (let i = 0; i < hash.length; i += 2) {
    hashBuffer[i / 2] = parseInt(hash.slice(i, i + 2), 16);
  }
  return _uint8ArrayToHex(hashBuffer.slice(0, 4)) + "-" + _uint8ArrayToHex(hashBuffer.slice(4, 6)) + "-" + _uint8ToHex(hashBuffer[6] & 15) + _uint8ToHex(hashBuffer[7]) + "-" + _uint8ToHex(hashBuffer[8] & 63 | 128) + _uint8ToHex(hashBuffer[9]) + "-" + _uint8ArrayToHex(hashBuffer.slice(10, 16));
}

// src/knowledge.ts
async function get(runtime, message) {
  if (!message?.content?.text) {
    logger_default.warn("Invalid message for knowledge query:", {
      message,
      content: message?.content,
      text: message?.content?.text
    });
    return [];
  }
  const processed = preprocess(message.content.text);
  logger_default.debug("Knowledge query:", {
    original: message.content.text,
    processed,
    length: processed?.length
  });
  if (!processed || processed.trim().length === 0) {
    logger_default.warn("Empty processed text for knowledge query");
    return [];
  }
  const embedding = await embed(runtime, processed);
  const fragments = await runtime.knowledgeManager.searchMemoriesByEmbedding(
    embedding,
    {
      roomId: message.agentId,
      count: 5,
      match_threshold: 0.1
    }
  );
  const uniqueSources = [
    ...new Set(
      fragments.map((memory) => {
        logger_default.log(
          `Matched fragment: ${memory.content.text} with similarity: ${memory.similarity}`
        );
        return memory.content.source;
      })
    )
  ];
  const knowledgeDocuments = await Promise.all(
    uniqueSources.map(
      (source) => runtime.documentsManager.getMemoryById(source)
    )
  );
  return knowledgeDocuments.filter((memory) => memory !== null).map((memory) => ({ id: memory.id, content: memory.content }));
}
async function set(runtime, item, chunkSize = 512, bleed = 20) {
  await runtime.documentsManager.createMemory({
    id: item.id,
    agentId: runtime.agentId,
    roomId: runtime.agentId,
    userId: runtime.agentId,
    createdAt: Date.now(),
    content: item.content,
    embedding: getEmbeddingZeroVector()
  });
  const preprocessed = preprocess(item.content.text);
  const fragments = await splitChunks(preprocessed, chunkSize, bleed);
  for (const fragment of fragments) {
    const embedding = await embed(runtime, fragment);
    await runtime.knowledgeManager.createMemory({
      // We namespace the knowledge base uuid to avoid id
      // collision with the document above.
      id: stringToUuid(item.id + fragment),
      roomId: runtime.agentId,
      agentId: runtime.agentId,
      userId: runtime.agentId,
      createdAt: Date.now(),
      content: {
        source: item.id,
        text: fragment
      },
      embedding
    });
  }
}
function preprocess(content) {
  logger_default.debug("Preprocessing text:", {
    input: content,
    length: content?.length
  });
  if (!content || typeof content !== "string") {
    logger_default.warn("Invalid input for preprocessing");
    return "";
  }
  return content.replace(/```[\s\S]*?```/g, "").replace(/`.*?`/g, "").replace(/#{1,6}\s*(.*)/g, "$1").replace(/!\[(.*?)\]\(.*?\)/g, "$1").replace(/\[(.*?)\]\(.*?\)/g, "$1").replace(/(https?:\/\/)?(www\.)?([^\s]+\.[^\s]+)/g, "$3").replace(/<@[!&]?\d+>/g, "").replace(/<[^>]*>/g, "").replace(/^\s*[-*_]{3,}\s*$/gm, "").replace(/\/\*[\s\S]*?\*\//g, "").replace(/\/\/.*/g, "").replace(/\s+/g, " ").replace(/\n{3,}/g, "\n\n").replace(/[^a-zA-Z0-9\s\-_./:?=&]/g, "").trim().toLowerCase();
}
var knowledge_default = {
  get,
  set,
  preprocess
};

// src/runtime.ts
var AgentRuntime = class {
  /**
   * Default count for recent messages to be kept in memory.
   * @private
   */
  #conversationLength = 32;
  /**
   * The ID of the agent
   */
  agentId;
  /**
   * The base URL of the server where the agent's requests are processed.
   */
  serverUrl = "http://localhost:7998";
  /**
   * The database adapter used for interacting with the database.
   */
  databaseAdapter;
  /**
   * Authentication token used for securing requests.
   */
  token;
  /**
   * Custom actions that the agent can perform.
   */
  actions = [];
  /**
   * Evaluators used to assess and guide the agent's responses.
   */
  evaluators = [];
  /**
   * Context providers used to provide context for message generation.
   */
  providers = [];
  plugins = [];
  /**
   * The model to use for generateText.
   */
  modelProvider;
  /**
   * The model to use for generateImage.
   */
  imageModelProvider;
  /**
   * Fetch function to use
   * Some environments may not have access to the global fetch function and need a custom fetch override.
   */
  fetch = fetch;
  /**
   * The character to use for the agent
   */
  character;
  /**
   * Store messages that are sent and received by the agent.
   */
  messageManager;
  /**
   * Store and recall descriptions of users based on conversations.
   */
  descriptionManager;
  /**
   * Manage the creation and recall of static information (documents, historical game lore, etc)
   */
  loreManager;
  /**
   * Hold large documents that can be referenced
   */
  documentsManager;
  /**
   * Searchable document fragments
   */
  knowledgeManager;
  services = /* @__PURE__ */ new Map();
  memoryManagers = /* @__PURE__ */ new Map();
  cacheManager;
  clients;
  registerMemoryManager(manager) {
    if (!manager.tableName) {
      throw new Error("Memory manager must have a tableName");
    }
    if (this.memoryManagers.has(manager.tableName)) {
      elizaLogger.warn(
        `Memory manager ${manager.tableName} is already registered. Skipping registration.`
      );
      return;
    }
    this.memoryManagers.set(manager.tableName, manager);
  }
  getMemoryManager(tableName) {
    return this.memoryManagers.get(tableName) || null;
  }
  getService(service) {
    const serviceInstance = this.services.get(service);
    if (!serviceInstance) {
      elizaLogger.error(`Service ${service} not found`);
      return null;
    }
    return serviceInstance;
  }
  async registerService(service) {
    const serviceType = service.serviceType;
    elizaLogger.log("Registering service:", serviceType);
    if (this.services.has(serviceType)) {
      elizaLogger.warn(
        `Service ${serviceType} is already registered. Skipping registration.`
      );
      return;
    }
    this.services.set(serviceType, service);
    elizaLogger.success(`Service ${serviceType} registered successfully`);
  }
  /**
   * Creates an instance of AgentRuntime.
   * @param opts - The options for configuring the AgentRuntime.
   * @param opts.conversationLength - The number of messages to hold in the recent message cache.
   * @param opts.token - The JWT token, can be a JWT token if outside worker, or an OpenAI token if inside worker.
   * @param opts.serverUrl - The URL of the worker.
   * @param opts.actions - Optional custom actions.
   * @param opts.evaluators - Optional custom evaluators.
   * @param opts.services - Optional custom services.
   * @param opts.memoryManagers - Optional custom memory managers.
   * @param opts.providers - Optional context providers.
   * @param opts.model - The model to use for generateText.
   * @param opts.embeddingModel - The model to use for embedding.
   * @param opts.agentId - Optional ID of the agent.
   * @param opts.databaseAdapter - The database adapter used for interacting with the database.
   * @param opts.fetch - Custom fetch function to use for making requests.
   */
  constructor(opts) {
    elizaLogger.info("Initializing AgentRuntime with options:", {
      character: opts.character?.name,
      modelProvider: opts.modelProvider,
      characterModelProvider: opts.character?.modelProvider
    });
    this.#conversationLength = opts.conversationLength ?? this.#conversationLength;
    if (!opts.databaseAdapter) {
      throw new Error("No database adapter provided");
    }
    this.databaseAdapter = opts.databaseAdapter;
    this.agentId = opts.character?.id ?? opts?.agentId ?? stringToUuid(opts.character?.name ?? uuidv4());
    this.character = opts.character || defaultCharacter;
    this.ensureRoomExists(this.agentId);
    this.ensureUserExists(
      this.agentId,
      this.character.name,
      this.character.name
    ).then(() => {
      this.ensureParticipantExists(this.agentId, this.agentId);
    });
    elizaLogger.success("Agent ID", this.agentId);
    this.fetch = opts.fetch ?? this.fetch;
    this.cacheManager = opts.cacheManager;
    this.messageManager = new MemoryManager({
      runtime: this,
      tableName: "messages"
    });
    this.descriptionManager = new MemoryManager({
      runtime: this,
      tableName: "descriptions"
    });
    this.loreManager = new MemoryManager({
      runtime: this,
      tableName: "lore"
    });
    this.documentsManager = new MemoryManager({
      runtime: this,
      tableName: "documents"
    });
    this.knowledgeManager = new MemoryManager({
      runtime: this,
      tableName: "fragments"
    });
    (opts.managers ?? []).forEach((manager) => {
      this.registerMemoryManager(manager);
    });
    (opts.services ?? []).forEach((service) => {
      this.registerService(service);
    });
    this.serverUrl = opts.serverUrl ?? this.serverUrl;
    elizaLogger.info("Setting model provider...");
    elizaLogger.info("Model Provider Selection:", {
      characterModelProvider: this.character.modelProvider,
      optsModelProvider: opts.modelProvider,
      currentModelProvider: this.modelProvider,
      finalSelection: this.character.modelProvider ?? opts.modelProvider ?? this.modelProvider
    });
    this.modelProvider = this.character.modelProvider ?? opts.modelProvider ?? this.modelProvider;
    this.imageModelProvider = this.character.imageModelProvider ?? this.modelProvider;
    elizaLogger.info("Selected model provider:", this.modelProvider);
    elizaLogger.info(
      "Selected image model provider:",
      this.imageModelProvider
    );
    if (!Object.values(ModelProviderName).includes(this.modelProvider)) {
      elizaLogger.error("Invalid model provider:", this.modelProvider);
      elizaLogger.error(
        "Available providers:",
        Object.values(ModelProviderName)
      );
      throw new Error(`Invalid model provider: ${this.modelProvider}`);
    }
    if (!this.serverUrl) {
      elizaLogger.warn("No serverUrl provided, defaulting to localhost");
    }
    this.token = opts.token;
    this.plugins = [
      ...opts.character?.plugins ?? [],
      ...opts.plugins ?? []
    ];
    this.plugins.forEach((plugin) => {
      plugin.actions?.forEach((action) => {
        this.registerAction(action);
      });
      plugin.evaluators?.forEach((evaluator) => {
        this.registerEvaluator(evaluator);
      });
      plugin.services?.forEach((service) => {
        this.registerService(service);
      });
      plugin.providers?.forEach((provider) => {
        this.registerContextProvider(provider);
      });
    });
    (opts.actions ?? []).forEach((action) => {
      this.registerAction(action);
    });
    (opts.providers ?? []).forEach((provider) => {
      this.registerContextProvider(provider);
    });
    (opts.evaluators ?? []).forEach((evaluator) => {
      this.registerEvaluator(evaluator);
    });
  }
  async initialize() {
    for (const [serviceType, service] of this.services.entries()) {
      try {
        await service.initialize(this);
        this.services.set(serviceType, service);
        elizaLogger.success(
          `Service ${serviceType} initialized successfully`
        );
      } catch (error) {
        elizaLogger.error(
          `Failed to initialize service ${serviceType}:`,
          error
        );
        throw error;
      }
    }
    for (const plugin of this.plugins) {
      if (plugin.services)
        await Promise.all(
          plugin.services?.map((service) => service.initialize(this))
        );
    }
    if (this.character && this.character.knowledge && this.character.knowledge.length > 0) {
      await this.processCharacterKnowledge(this.character.knowledge);
    }
  }
  async stop() {
    elizaLogger.debug("runtime::stop - character", this.character);
    for (const cStr in this.clients) {
      const c = this.clients[cStr];
      elizaLogger.log("runtime::stop - requesting", cStr, "client stop for", this.character.name);
      c.stop();
    }
  }
  /**
   * Processes character knowledge by creating document memories and fragment memories.
   * This function takes an array of knowledge items, creates a document memory for each item if it doesn't exist,
   * then chunks the content into fragments, embeds each fragment, and creates fragment memories.
   * @param knowledge An array of knowledge items containing id, path, and content.
   */
  async processCharacterKnowledge(items) {
    for (const item of items) {
      const knowledgeId = stringToUuid(item);
      const existingDocument = await this.documentsManager.getMemoryById(knowledgeId);
      if (existingDocument) {
        continue;
      }
      elizaLogger.info(
        "Processing knowledge for ",
        this.character.name,
        " - ",
        item.slice(0, 100)
      );
      await knowledge_default.set(this, {
        id: knowledgeId,
        content: {
          text: item
        }
      });
    }
  }
  getSetting(key) {
    if (this.character.settings?.secrets?.[key]) {
      return this.character.settings.secrets[key];
    }
    if (this.character.settings?.[key]) {
      return this.character.settings[key];
    }
    if (settings_default[key]) {
      return settings_default[key];
    }
    return null;
  }
  /**
   * Get the number of messages that are kept in the conversation buffer.
   * @returns The number of recent messages to be kept in memory.
   */
  getConversationLength() {
    return this.#conversationLength;
  }
  /**
   * Register an action for the agent to perform.
   * @param action The action to register.
   */
  registerAction(action) {
    elizaLogger.success(`Registering action: ${action.name}`);
    this.actions.push(action);
  }
  /**
   * Register an evaluator to assess and guide the agent's responses.
   * @param evaluator The evaluator to register.
   */
  registerEvaluator(evaluator) {
    this.evaluators.push(evaluator);
  }
  /**
   * Register a context provider to provide context for message generation.
   * @param provider The context provider to register.
   */
  registerContextProvider(provider) {
    this.providers.push(provider);
  }
  /**
   * Process the actions of a message.
   * @param message The message to process.
   * @param content The content of the message to process actions from.
   */
  async processActions(message, responses, state, callback) {
    for (const response of responses) {
      if (!response.content?.action) {
        elizaLogger.warn("No action found in the response content.");
        continue;
      }
      const normalizedAction = response.content.action.toLowerCase().replace("_", "");
      elizaLogger.success(`Normalized action: ${normalizedAction}`);
      let action = this.actions.find(
        (a) => a.name.toLowerCase().replace("_", "").includes(normalizedAction) || normalizedAction.includes(
          a.name.toLowerCase().replace("_", "")
        )
      );
      if (!action) {
        elizaLogger.info("Attempting to find action in similes.");
        for (const _action of this.actions) {
          const simileAction = _action.similes.find(
            (simile) => simile.toLowerCase().replace("_", "").includes(normalizedAction) || normalizedAction.includes(
              simile.toLowerCase().replace("_", "")
            )
          );
          if (simileAction) {
            action = _action;
            elizaLogger.success(
              `Action found in similes: ${action.name}`
            );
            break;
          }
        }
      }
      if (!action) {
        elizaLogger.error(
          "No action found for",
          response.content.action
        );
        continue;
      }
      if (!action.handler) {
        elizaLogger.error(`Action ${action.name} has no handler.`);
        continue;
      }
      try {
        elizaLogger.info(
          `Executing handler for action: ${action.name}`
        );
        await action.handler(this, message, state, {}, callback);
      } catch (error) {
        elizaLogger.error(error);
      }
    }
  }
  /**
   * Evaluate the message and state using the registered evaluators.
   * @param message The message to evaluate.
   * @param state The state of the agent.
   * @param didRespond Whether the agent responded to the message.~
   * @param callback The handler callback
   * @returns The results of the evaluation.
   */
  async evaluate(message, state, didRespond, callback) {
    const evaluatorPromises = this.evaluators.map(
      async (evaluator) => {
        elizaLogger.log("Evaluating", evaluator.name);
        if (!evaluator.handler) {
          return null;
        }
        if (!didRespond && !evaluator.alwaysRun) {
          return null;
        }
        const result2 = await evaluator.validate(this, message, state);
        if (result2) {
          return evaluator;
        }
        return null;
      }
    );
    const resolvedEvaluators = await Promise.all(evaluatorPromises);
    const evaluatorsData = resolvedEvaluators.filter(Boolean);
    if (evaluatorsData.length === 0) {
      return [];
    }
    const context = composeContext({
      state: {
        ...state,
        evaluators: formatEvaluators(evaluatorsData),
        evaluatorNames: formatEvaluatorNames(evaluatorsData)
      },
      template: this.character.templates?.evaluationTemplate || evaluationTemplate
    });
    const result = await generateText({
      runtime: this,
      context,
      modelClass: "small" /* SMALL */
    });
    const evaluators = parseJsonArrayFromText(
      result
    );
    for (const evaluator of this.evaluators) {
      if (!evaluators.includes(evaluator.name)) continue;
      if (evaluator.handler)
        await evaluator.handler(this, message, state, {}, callback);
    }
    return evaluators;
  }
  /**
   * Ensure the existence of a participant in the room. If the participant does not exist, they are added to the room.
   * @param userId - The user ID to ensure the existence of.
   * @throws An error if the participant cannot be added.
   */
  async ensureParticipantExists(userId, roomId) {
    const participants = await this.databaseAdapter.getParticipantsForAccount(userId);
    if (participants?.length === 0) {
      await this.databaseAdapter.addParticipant(userId, roomId);
    }
  }
  /**
   * Ensure the existence of a user in the database. If the user does not exist, they are added to the database.
   * @param userId - The user ID to ensure the existence of.
   * @param userName - The user name to ensure the existence of.
   * @returns
   */
  async ensureUserExists(userId, userName, name, email, source) {
    const account = await this.databaseAdapter.getAccountById(userId);
    if (!account) {
      await this.databaseAdapter.createAccount({
        id: userId,
        name: name || userName || "Unknown User",
        username: userName || name || "Unknown",
        email: email || (userName || "Bot") + "@" + source || "Unknown",
        // Temporary
        details: { summary: "" }
      });
      elizaLogger.success(`User ${userName} created successfully.`);
    }
  }
  async ensureParticipantInRoom(userId, roomId) {
    const participants = await this.databaseAdapter.getParticipantsForRoom(roomId);
    if (!participants.includes(userId)) {
      await this.databaseAdapter.addParticipant(userId, roomId);
      if (userId === this.agentId) {
        elizaLogger.log(
          `Agent ${this.character.name} linked to room ${roomId} successfully.`
        );
      } else {
        elizaLogger.log(
          `User ${userId} linked to room ${roomId} successfully.`
        );
      }
    }
  }
  async ensureConnection(userId, roomId, userName, userScreenName, source) {
    await Promise.all([
      this.ensureUserExists(
        this.agentId,
        this.character.name ?? "Agent",
        this.character.name ?? "Agent",
        source
      ),
      this.ensureUserExists(
        userId,
        userName ?? "User" + userId,
        userScreenName ?? "User" + userId,
        source
      ),
      this.ensureRoomExists(roomId)
    ]);
    await Promise.all([
      this.ensureParticipantInRoom(userId, roomId),
      this.ensureParticipantInRoom(this.agentId, roomId)
    ]);
  }
  /**
   * Ensure the existence of a room between the agent and a user. If no room exists, a new room is created and the user
   * and agent are added as participants. The room ID is returned.
   * @param userId - The user ID to create a room with.
   * @returns The room ID of the room between the agent and the user.
   * @throws An error if the room cannot be created.
   */
  async ensureRoomExists(roomId) {
    const room = await this.databaseAdapter.getRoom(roomId);
    if (!room) {
      await this.databaseAdapter.createRoom(roomId);
      elizaLogger.log(`Room ${roomId} created successfully.`);
    }
  }
  /**
   * Compose the state of the agent into an object that can be passed or used for response generation.
   * @param message The message to compose the state from.
   * @returns The state of the agent.
   */
  async composeState(message, additionalKeys = {}) {
    const { userId, roomId } = message;
    const conversationLength = this.getConversationLength();
    const [actorsData, recentMessagesData, goalsData] = await Promise.all([
      getActorDetails({ runtime: this, roomId }),
      this.messageManager.getMemories({
        roomId,
        count: conversationLength,
        unique: false
      }),
      getGoals({
        runtime: this,
        count: 10,
        onlyInProgress: false,
        roomId
      })
    ]);
    const goals = formatGoalsAsString({ goals: goalsData });
    const actors = formatActors({ actors: actorsData ?? [] });
    const recentMessages = formatMessages({
      messages: recentMessagesData,
      actors: actorsData
    });
    const recentPosts = formatPosts({
      messages: recentMessagesData,
      actors: actorsData,
      conversationHeader: false
    });
    const senderName = actorsData?.find(
      (actor) => actor.id === userId
    )?.name;
    const agentName = actorsData?.find((actor) => actor.id === this.agentId)?.name || this.character.name;
    let allAttachments = message.content.attachments || [];
    if (recentMessagesData && Array.isArray(recentMessagesData)) {
      const lastMessageWithAttachment = recentMessagesData.find(
        (msg) => msg.content.attachments && msg.content.attachments.length > 0
      );
      if (lastMessageWithAttachment) {
        const lastMessageTime = lastMessageWithAttachment.createdAt;
        const oneHourBeforeLastMessage = lastMessageTime - 60 * 60 * 1e3;
        allAttachments = recentMessagesData.reverse().map((msg) => {
          const msgTime = msg.createdAt ?? Date.now();
          const isWithinTime = msgTime >= oneHourBeforeLastMessage;
          const attachments = msg.content.attachments || [];
          if (!isWithinTime) {
            attachments.forEach((attachment) => {
              attachment.text = "[Hidden]";
            });
          }
          return attachments;
        }).flat();
      }
    }
    const formattedAttachments = allAttachments.map(
      (attachment) => `ID: ${attachment.id}
Name: ${attachment.title}
URL: ${attachment.url}
Type: ${attachment.source}
Description: ${attachment.description}
Text: ${attachment.text}
  `
    ).join("\n");
    let lore = "";
    if (this.character.lore && this.character.lore.length > 0) {
      const shuffledLore = [...this.character.lore].sort(
        () => Math.random() - 0.5
      );
      const selectedLore = shuffledLore.slice(0, 10);
      lore = selectedLore.join("\n");
    }
    const formattedCharacterPostExamples = this.character.postExamples.sort(() => 0.5 - Math.random()).map((post) => {
      const messageString = `${post}`;
      return messageString;
    }).slice(0, 50).join("\n");
    const formattedCharacterMessageExamples = this.character.messageExamples.sort(() => 0.5 - Math.random()).slice(0, 5).map((example) => {
      const exampleNames = Array.from(
        { length: 5 },
        () => uniqueNamesGenerator4({ dictionaries: [names4] })
      );
      return example.map((message2) => {
        let messageString = `${message2.user}: ${message2.content.text}`;
        exampleNames.forEach((name, index) => {
          const placeholder = `{{user${index + 1}}}`;
          messageString = messageString.replaceAll(
            placeholder,
            name
          );
        });
        return messageString;
      }).join("\n");
    }).join("\n\n");
    const getRecentInteractions = async (userA, userB) => {
      const rooms = await this.databaseAdapter.getRoomsForParticipants([
        userA,
        userB
      ]);
      const existingMemories = await this.messageManager.getMemoriesByRoomIds({
        // filter out the current room id from rooms
        roomIds: rooms.filter((room) => room !== roomId)
      });
      existingMemories.sort((a, b) => b.createdAt - a.createdAt);
      const recentInteractionsData = existingMemories.slice(0, 20);
      return recentInteractionsData;
    };
    const recentInteractions = userId !== this.agentId ? await getRecentInteractions(userId, this.agentId) : [];
    const getRecentMessageInteractions = async (recentInteractionsData) => {
      const formattedInteractions = await Promise.all(
        recentInteractionsData.map(async (message2) => {
          const isSelf = message2.userId === this.agentId;
          let sender;
          if (isSelf) {
            sender = this.character.name;
          } else {
            const accountId = await this.databaseAdapter.getAccountById(
              message2.userId
            );
            sender = accountId?.username || "unknown";
          }
          return `${sender}: ${message2.content.text}`;
        })
      );
      return formattedInteractions.join("\n");
    };
    const formattedMessageInteractions = await getRecentMessageInteractions(recentInteractions);
    const getRecentPostInteractions = async (recentInteractionsData, actors2) => {
      const formattedInteractions = formatPosts({
        messages: recentInteractionsData,
        actors: actors2,
        conversationHeader: true
      });
      return formattedInteractions;
    };
    const formattedPostInteractions = await getRecentPostInteractions(
      recentInteractions,
      actorsData
    );
    let bio = this.character.bio || "";
    if (Array.isArray(bio)) {
      bio = bio.sort(() => 0.5 - Math.random()).slice(0, 3).join(" ");
    }
    const knowledegeData = await knowledge_default.get(this, message);
    const formattedKnowledge = formatKnowledge(knowledegeData);
    const initialState = {
      agentId: this.agentId,
      agentName,
      bio,
      lore,
      adjective: this.character.adjectives && this.character.adjectives.length > 0 ? this.character.adjectives[Math.floor(
        Math.random() * this.character.adjectives.length
      )] : "",
      knowledge: formattedKnowledge,
      knowledgeData: knowledegeData,
      // Recent interactions between the sender and receiver, formatted as messages
      recentMessageInteractions: formattedMessageInteractions,
      // Recent interactions between the sender and receiver, formatted as posts
      recentPostInteractions: formattedPostInteractions,
      // Raw memory[] array of interactions
      recentInteractionsData: recentInteractions,
      // randomly pick one topic
      topic: this.character.topics && this.character.topics.length > 0 ? this.character.topics[Math.floor(
        Math.random() * this.character.topics.length
      )] : null,
      topics: this.character.topics && this.character.topics.length > 0 ? `${this.character.name} is interested in ` + this.character.topics.sort(() => 0.5 - Math.random()).slice(0, 5).map((topic, index) => {
        if (index === this.character.topics.length - 2) {
          return topic + " and ";
        }
        if (index === this.character.topics.length - 1) {
          return topic;
        }
        return topic + ", ";
      }).join("") : "",
      characterPostExamples: formattedCharacterPostExamples && formattedCharacterPostExamples.replaceAll("\n", "").length > 0 ? addHeader(
        `# Example Posts for ${this.character.name}`,
        formattedCharacterPostExamples
      ) : "",
      characterMessageExamples: formattedCharacterMessageExamples && formattedCharacterMessageExamples.replaceAll("\n", "").length > 0 ? addHeader(
        `# Example Conversations for ${this.character.name}`,
        formattedCharacterMessageExamples
      ) : "",
      messageDirections: this.character?.style?.all?.length > 0 || this.character?.style?.chat.length > 0 ? addHeader(
        "# Message Directions for " + this.character.name,
        (() => {
          const all = this.character?.style?.all || [];
          const chat = this.character?.style?.chat || [];
          return [...all, ...chat].join("\n");
        })()
      ) : "",
      postDirections: this.character?.style?.all?.length > 0 || this.character?.style?.post.length > 0 ? addHeader(
        "# Post Directions for " + this.character.name,
        (() => {
          const all = this.character?.style?.all || [];
          const post = this.character?.style?.post || [];
          return [...all, ...post].join("\n");
        })()
      ) : "",
      //old logic left in for reference
      //food for thought. how could we dynamically decide what parts of the character to add to the prompt other than random? rag? prompt the llm to decide?
      /*
      postDirections:
          this.character?.style?.all?.length > 0 ||
          this.character?.style?.post.length > 0
              ? addHeader(
                      "# Post Directions for " + this.character.name,
                      (() => {
                          const all = this.character?.style?.all || [];
                          const post = this.character?.style?.post || [];
                          const shuffled = [...all, ...post].sort(
                              () => 0.5 - Math.random()
                          );
                          return shuffled
                              .slice(0, conversationLength / 2)
                              .join("\n");
                      })()
                  )
              : "",*/
      // Agent runtime stuff
      senderName,
      actors: actors && actors.length > 0 ? addHeader("# Actors", actors) : "",
      actorsData,
      roomId,
      goals: goals && goals.length > 0 ? addHeader(
        "# Goals\n{{agentName}} should prioritize accomplishing the objectives that are in progress.",
        goals
      ) : "",
      goalsData,
      recentMessages: recentMessages && recentMessages.length > 0 ? addHeader("# Conversation Messages", recentMessages) : "",
      recentPosts: recentPosts && recentPosts.length > 0 ? addHeader("# Posts in Thread", recentPosts) : "",
      recentMessagesData,
      attachments: formattedAttachments && formattedAttachments.length > 0 ? addHeader("# Attachments", formattedAttachments) : "",
      ...additionalKeys
    };
    const actionPromises = this.actions.map(async (action) => {
      const result = await action.validate(this, message, initialState);
      if (result) {
        return action;
      }
      return null;
    });
    const evaluatorPromises = this.evaluators.map(async (evaluator) => {
      const result = await evaluator.validate(
        this,
        message,
        initialState
      );
      if (result) {
        return evaluator;
      }
      return null;
    });
    const [resolvedEvaluators, resolvedActions, providers] = await Promise.all([
      Promise.all(evaluatorPromises),
      Promise.all(actionPromises),
      getProviders(this, message, initialState)
    ]);
    const evaluatorsData = resolvedEvaluators.filter(
      Boolean
    );
    const actionsData = resolvedActions.filter(Boolean);
    const actionState = {
      actionNames: "Possible response actions: " + formatActionNames(actionsData),
      actions: actionsData.length > 0 ? addHeader(
        "# Available Actions",
        formatActions(actionsData)
      ) : "",
      actionExamples: actionsData.length > 0 ? addHeader(
        "# Action Examples",
        composeActionExamples(actionsData, 10)
      ) : "",
      evaluatorsData,
      evaluators: evaluatorsData.length > 0 ? formatEvaluators(evaluatorsData) : "",
      evaluatorNames: evaluatorsData.length > 0 ? formatEvaluatorNames(evaluatorsData) : "",
      evaluatorExamples: evaluatorsData.length > 0 ? formatEvaluatorExamples(evaluatorsData) : "",
      providers: addHeader(
        `# Additional Information About ${this.character.name} and The World`,
        providers
      )
    };
    return { ...initialState, ...actionState };
  }
  async updateRecentMessageState(state) {
    const conversationLength = this.getConversationLength();
    const recentMessagesData = await this.messageManager.getMemories({
      roomId: state.roomId,
      count: conversationLength,
      unique: false
    });
    const recentMessages = formatMessages({
      actors: state.actorsData ?? [],
      messages: recentMessagesData.map((memory) => {
        const newMemory = { ...memory };
        delete newMemory.embedding;
        return newMemory;
      })
    });
    let allAttachments = [];
    if (recentMessagesData && Array.isArray(recentMessagesData)) {
      const lastMessageWithAttachment = recentMessagesData.find(
        (msg) => msg.content.attachments && msg.content.attachments.length > 0
      );
      if (lastMessageWithAttachment) {
        const lastMessageTime = lastMessageWithAttachment.createdAt;
        const oneHourBeforeLastMessage = lastMessageTime - 60 * 60 * 1e3;
        allAttachments = recentMessagesData.filter((msg) => {
          const msgTime = msg.createdAt;
          return msgTime >= oneHourBeforeLastMessage;
        }).flatMap((msg) => msg.content.attachments || []);
      }
    }
    const formattedAttachments = allAttachments.map(
      (attachment) => `ID: ${attachment.id}
Name: ${attachment.title}
URL: ${attachment.url}
Type: ${attachment.source}
Description: ${attachment.description}
Text: ${attachment.text}
    `
    ).join("\n");
    return {
      ...state,
      recentMessages: addHeader(
        "# Conversation Messages",
        recentMessages
      ),
      recentMessagesData,
      attachments: formattedAttachments
    };
  }
};
var formatKnowledge = (knowledge) => {
  return knowledge.map((knowledge2) => `- ${knowledge2.content.text}`).join("\n");
};

// src/environment.ts
import { z } from "zod";
var envSchema = z.object({
  // API Keys with specific formats
  OPENAI_API_KEY: z.string().startsWith("sk-", "OpenAI API key must start with 'sk-'"),
  REDPILL_API_KEY: z.string().min(1, "REDPILL API key is required"),
  GROK_API_KEY: z.string().min(1, "GROK API key is required"),
  GROQ_API_KEY: z.string().startsWith("gsk_", "GROQ API key must start with 'gsk_'"),
  OPENROUTER_API_KEY: z.string().min(1, "OpenRouter API key is required"),
  GOOGLE_GENERATIVE_AI_API_KEY: z.string().min(1, "Gemini API key is required"),
  ELEVENLABS_XI_API_KEY: z.string().min(1, "ElevenLabs API key is required")
});
function validateEnv() {
  try {
    return envSchema.parse(process.env);
  } catch (error) {
    if (error instanceof z.ZodError) {
      const errorMessages = error.errors.map((err) => `${err.path}: ${err.message}`).join("\n");
      throw new Error(`Environment validation failed:
${errorMessages}`);
    }
    throw error;
  }
}
var MessageExampleSchema = z.object({
  user: z.string(),
  content: z.object({
    text: z.string(),
    action: z.string().optional(),
    source: z.string().optional(),
    url: z.string().optional(),
    inReplyTo: z.string().uuid().optional(),
    attachments: z.array(z.any()).optional()
  }).and(z.record(z.string(), z.unknown()))
  // For additional properties
});
var PluginSchema = z.object({
  name: z.string(),
  description: z.string(),
  actions: z.array(z.any()).optional(),
  providers: z.array(z.any()).optional(),
  evaluators: z.array(z.any()).optional(),
  services: z.array(z.any()).optional(),
  clients: z.array(z.any()).optional()
});
var CharacterSchema = z.object({
  id: z.string().uuid().optional(),
  name: z.string(),
  system: z.string().optional(),
  modelProvider: z.nativeEnum(ModelProviderName),
  modelEndpointOverride: z.string().optional(),
  templates: z.record(z.string()).optional(),
  bio: z.union([z.string(), z.array(z.string())]),
  lore: z.array(z.string()),
  messageExamples: z.array(z.array(MessageExampleSchema)),
  postExamples: z.array(z.string()),
  topics: z.array(z.string()),
  adjectives: z.array(z.string()),
  knowledge: z.array(z.string()).optional(),
  clients: z.array(z.nativeEnum(Clients)),
  plugins: z.union([
    z.array(z.string()),
    z.array(PluginSchema)
  ]),
  settings: z.object({
    secrets: z.record(z.string()).optional(),
    voice: z.object({
      model: z.string().optional(),
      url: z.string().optional()
    }).optional(),
    model: z.string().optional(),
    embeddingModel: z.string().optional()
  }).optional(),
  clientConfig: z.object({
    discord: z.object({
      shouldIgnoreBotMessages: z.boolean().optional(),
      shouldIgnoreDirectMessages: z.boolean().optional()
    }).optional(),
    telegram: z.object({
      shouldIgnoreBotMessages: z.boolean().optional(),
      shouldIgnoreDirectMessages: z.boolean().optional()
    }).optional()
  }).optional(),
  style: z.object({
    all: z.array(z.string()),
    chat: z.array(z.string()),
    post: z.array(z.string())
  }),
  twitterProfile: z.object({
    username: z.string(),
    screenName: z.string(),
    bio: z.string(),
    nicknames: z.array(z.string()).optional()
  }).optional(),
  nft: z.object({
    prompt: z.string().optional()
  }).optional()
});
function validateCharacterConfig(json) {
  try {
    return CharacterSchema.parse(json);
  } catch (error) {
    if (error instanceof z.ZodError) {
      const errorMessages = error.errors.map((err) => `${err.path.join(".")}: ${err.message}`).join("\n");
      throw new Error(
        `Character configuration validation failed:
${errorMessages}`
      );
    }
    throw error;
  }
}

// src/cache.ts
import path4 from "path";
import fs2 from "fs/promises";
var MemoryCacheAdapter = class {
  data;
  constructor(initalData) {
    this.data = initalData ?? /* @__PURE__ */ new Map();
  }
  async get(key) {
    return this.data.get(key);
  }
  async set(key, value) {
    this.data.set(key, value);
  }
  async delete(key) {
    this.data.delete(key);
  }
};
var FsCacheAdapter = class {
  constructor(dataDir) {
    this.dataDir = dataDir;
  }
  async get(key) {
    try {
      return await fs2.readFile(path4.join(this.dataDir, key), "utf8");
    } catch {
      return void 0;
    }
  }
  async set(key, value) {
    try {
      const filePath = path4.join(this.dataDir, key);
      await fs2.mkdir(path4.dirname(filePath), { recursive: true });
      await fs2.writeFile(filePath, value, "utf8");
    } catch (error) {
      console.error(error);
    }
  }
  async delete(key) {
    try {
      const filePath = path4.join(this.dataDir, key);
      await fs2.unlink(filePath);
    } catch {
    }
  }
};
var DbCacheAdapter = class {
  constructor(db, agentId) {
    this.db = db;
    this.agentId = agentId;
  }
  async get(key) {
    return this.db.getCache({ agentId: this.agentId, key });
  }
  async set(key, value) {
    await this.db.setCache({ agentId: this.agentId, key, value });
  }
  async delete(key) {
    await this.db.deleteCache({ agentId: this.agentId, key });
  }
};
var CacheManager = class {
  adapter;
  constructor(adapter) {
    this.adapter = adapter;
  }
  async get(key) {
    const data = await this.adapter.get(key);
    if (data) {
      const { value, expires } = JSON.parse(data);
      if (!expires || expires > Date.now()) {
        return value;
      }
      this.adapter.delete(key).catch(() => {
      });
    }
    return void 0;
  }
  async set(key, value, opts) {
    return this.adapter.set(
      key,
      JSON.stringify({ value, expires: opts?.expires ?? 0 })
    );
  }
  async delete(key) {
    return this.adapter.delete(key);
  }
};
export {
  AgentRuntime,
  CacheManager,
  CacheStore,
  CharacterSchema,
  Clients,
  DatabaseAdapter,
  DbCacheAdapter,
  EmbeddingProvider,
  FsCacheAdapter,
  GoalStatus,
  LoggingLevel,
  MemoryCacheAdapter,
  MemoryManager,
  ModelClass,
  ModelProviderName,
  Service,
  ServiceType,
  addHeader,
  booleanFooter,
  composeActionExamples,
  composeContext,
  composeRandomUser,
  configureSettings,
  createGoal,
  createRelationship,
  defaultCharacter,
  elizaLogger,
  embed,
  envSchema,
  evaluationTemplate,
  findNearestEnvFile,
  formatActionNames,
  formatActions,
  formatActors,
  formatEvaluatorExampleDescriptions,
  formatEvaluatorExamples,
  formatEvaluatorNames,
  formatEvaluators,
  formatGoalsAsString,
  formatMessages,
  formatPosts,
  formatRelationships,
  formatTimestamp,
  generateCaption,
  generateImage,
  generateMessageResponse,
  generateObject,
  generateObjectArray,
  generateObjectDeprecated,
  generateShouldRespond,
  generateText,
  generateTextArray,
  generateTrueOrFalse,
  generateTweetActions,
  generateWebSearch,
  getActorDetails,
  getEmbeddingConfig,
  getEmbeddingType,
  getEmbeddingZeroVector,
  getEndpoint,
  getEnvVariable,
  getGoals,
  getModel,
  getProviders,
  getRelationship,
  getRelationships,
  handleProvider,
  hasEnvVariable,
  knowledge_default as knowledge,
  loadEnvConfig,
  messageCompletionFooter,
  models,
  parseActionResponseFromText,
  parseBooleanFromText,
  parseJSONObjectFromText,
  parseJsonArrayFromText,
  parseShouldRespondFromText,
  postActionResponseFooter,
  settings,
  shouldRespondFooter,
  splitChunks,
  stringArrayFooter,
  stringToUuid,
  trimTokens,
  updateGoal,
  validateCharacterConfig,
  validateEnv
};
//# sourceMappingURL=index.js.map